{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import time\n",
    "\n",
    "from ImportData2D import get_label, getFilename, get_file_name\n",
    "from stlSlicer import getSlicer, slicecoor, rotatestl\n",
    "from imgSave import saveplot\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(125)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = getLabel(\"Occ_Sum\", \"median\", True, False,False)\n",
    "print(type(label[0]))\n",
    "label = getLabel(\"Occ_Sum\", \"median\", True, True,False)\n",
    "print(type(label[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "test_name = \"prep_test.tfrecords\"\n",
    "test_name = os.path.join(\"data\", test_name)\n",
    "print(test_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "A = './data/preparation_181_data_test_eval_address.txt'\n",
    "print(os.path.abspath(A))\n",
    "print(os.path.isfile(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_grouped_address = ['/home/pasin/Documents/Google_Drive/Aa_TIT_LAB_Comp/Library/Tooth/Tooth/Model/my2DCNN/data/cross_section/PreparationScan_0_84140','/home/pasin/Documents/Google_Drive/Aa_TIT_LAB_Comp/Library/Tooth/Tooth/Model/my2DCNN/data/cross_section/PreparationScan_1_95138_0','/home/pasin/Documents/Google_Drive/Aa_TIT_LAB_Comp/Library/Tooth/Tooth/Model/my2DCNN/data/cross_section/PreparationScan_1_304138_45']\n",
    "grouped_address = [1,2,3]\n",
    "# define an empty list\n",
    "train_address = []\n",
    "\n",
    "# open file and read the content in a list\n",
    "with open(A, 'r') as filehandle:  \n",
    "    for line in filehandle:\n",
    "        # remove linebreak which is the last character of the string\n",
    "        current_name = line[:-1]\n",
    "        # print(current_name)\n",
    "        for i, name in enumerate(example_grouped_address):\n",
    "            \n",
    "            if current_name in name:\n",
    "                train_address.append(grouped_address[i])\n",
    "                grouped_address.remove(grouped_address[i])\n",
    "                example_grouped_address.remove(example_grouped_address[i])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(train_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "B = ast.literal_eval(error_file_names[0])\n",
    "print(B[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_address = ['abc_0_0','abc_0_90','abc_0_45','abc_0_135','abc_1_0','abc_1_45','abc_1_90','abc_1_70']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_address.sort()\n",
    "print([s for s in train_address if 'abc_0' in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import csv\n",
    "A = {'learning_rate':0.00009123,'keep_prob':0.25, 'activation':tf.nn.relu, 'channels':[16,16,32]}\n",
    "# A = {'a':'b','c':'d'}\n",
    "\n",
    "with open(\"output\"+\".csv\", \"w\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for key, val in A.items():\n",
    "        writer.writerow([key, val])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000,)\n",
      "(51428, 784)\n"
     ]
    }
   ],
   "source": [
    "# You can add the parameter data_home to wherever to where you want to download your data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "((train_data, train_labels),(eval_data, eval_labels)) = tf.keras.datasets.mnist.load_data()\n",
    "train_data = train_data / np.float32(255)\n",
    "train_data = train_data.reshape(train_data.shape[0], -1)\n",
    "print(train_data.shape)\n",
    "train_labels = train_labels.astype(np.int32)  # not required\n",
    "print(train_labels.shape)\n",
    "# test_size: what proportion of original data is used for test set\n",
    "train_img, test_img, train_lbl, test_lbl = train_test_split(\n",
    "    train_data, train_labels, test_size=1/7.0, random_state=0)\n",
    "print(train_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the Data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training set only.\n",
    "scaler.fit(train_img)\n",
    "\n",
    "# Apply transform to both the training set and the test set.\n",
    "train_img = scaler.transform(train_img)\n",
    "test_img = scaler.transform(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pasin/anaconda3/envs/ps36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/pasin/anaconda3/envs/ps36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/pasin/anaconda3/envs/ps36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/pasin/anaconda3/envs/ps36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/pasin/anaconda3/envs/ps36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/pasin/anaconda3/envs/ps36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/pasin/anaconda3/envs/ps36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/pasin/anaconda3/envs/ps36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/pasin/anaconda3/envs/ps36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/pasin/anaconda3/envs/ps36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/pasin/anaconda3/envs/ps36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3, 6, 6, 6, 0, 3, 6, 2, 5, 6], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "pca = PCA(.95)\n",
    "pca.fit(train_img)\n",
    "pca.n_components_\n",
    "train_img = pca.transform(train_img)\n",
    "test_img = pca.transform(test_img)\n",
    "\n",
    "# all parameters not specified are set to their defaults\n",
    "# default solver is incredibly slow thats why we change it\n",
    "# solver = 'lbfgs'\n",
    "logisticRegr = LogisticRegression(solver = 'lbfgs')\n",
    "\n",
    "logisticRegr.fit(train_img, train_lbl)\n",
    "\n",
    "# Returns a NumPy Array\n",
    "# Predict for One Observation (image)\n",
    "logisticRegr.predict(test_img[0].reshape(1,-1))\n",
    "\n",
    "# Predict for Multiple Observations (images) at Once\n",
    "logisticRegr.predict(test_img[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.922071861875875\n"
     ]
    }
   ],
   "source": [
    "score = logisticRegr.score(test_img, test_lbl)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# tf.enable_eager_execution()\n",
    "import numpy as np\n",
    "\n",
    "# Fixed parameter\n",
    "numdegree = 4  # Number of rotations\n",
    "image_height = 240  # Used for cropping\n",
    "image_width = 360  # Used for cropping\n",
    "\n",
    "\n",
    "# Import tfrecord to dataset\n",
    "def deserialize(example):\n",
    "    feature = {'label': tf.FixedLenFeature([], tf.int64)}\n",
    "    for i in range(numdegree):\n",
    "        feature['img' + str(i)] = tf.FixedLenFeature([], tf.string)\n",
    "    return tf.parse_single_example(example, feature)\n",
    "\n",
    "\n",
    "def decode(data_dict):\n",
    "    if numdegree != 4:\n",
    "        raise Exception('Number of degree specified is not compatible, edit code')\n",
    "    # Create initial image, then stacking it\n",
    "    image_decoded = []\n",
    "\n",
    "    # Stacking the rest\n",
    "    for i in range(0, numdegree):\n",
    "        img = data_dict['img' + str(i)]\n",
    "        file_decoded = tf.image.decode_png(img, channels=1)\n",
    "        file_cropped = tf.squeeze(tf.image.resize_image_with_crop_or_pad(file_decoded, image_height, image_width))\n",
    "        image_decoded.append(file_cropped)\n",
    "\n",
    "    image_stacked = tf.stack([image_decoded[0], image_decoded[1], image_decoded[2], image_decoded[3]], axis=2)\n",
    "    image_stacked = tf.cast(image_stacked, tf.float32)\n",
    "    label = data_dict['label']\n",
    "    # output = (image_stacked, label)\n",
    "    # return {'images': image_stacked, 'label': label}  # Output is [Channel, Height, Width]\n",
    "    return image_stacked, label\n",
    "\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def _float_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def get_data_from_path(data_path):\n",
    "    dataset = tf.data.TFRecordDataset(data_path)\n",
    "    #print(dataset)\n",
    "    dataset = dataset.map(deserialize)\n",
    "    dataset = dataset.map(decode)\n",
    "    \n",
    "    max_elems = np.iinfo(np.int64).max\n",
    "    dataset = dataset.batch(1000, drop_remainder=False)\n",
    "    whole_dataset_tensors = tf.data.experimental.get_single_element(dataset)\n",
    "    with tf.Session() as sess:\n",
    "        whole_dataset_arrays = sess.run(whole_dataset_tensors)\n",
    "    return whole_dataset_arrays\n",
    "    \n",
    "    '''\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    next_example, next_label = iterator.get_next()\n",
    "    with tf.Session() as sess:\n",
    "        example = sess.run(next_example)\n",
    "        label = sess.run(next_label)\n",
    "    return example, label\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8  8  8  8  8  8 10  6 10  8  8  8  6 10  6 10  6  8  8 10  8  6  8  8\n",
      "  8  6  8  6  6 10 10  8  6  8  8  8  8  8  8  8  6  8  8  4  6  6  8  8\n",
      "  8  6  6  6  8 10  8  6  6  8  8  6  6  6  4  8  8  8  8  8  8  8 10  8\n",
      " 10  7  6 10  8  8  8  8  6  8  8  8  8 10  8 10  6 10  6  8  8  8 10  4\n",
      "  8  8  8  8 10  6  8  8  8  8  8  6 10  8  8 10 10  9  6  8  6  9  8 10\n",
      "  6  8  6  6  8 10  8  8  8  8  8  6  6 10  6  6  6  8  8  6  6  8 10 10\n",
      "  8]\n"
     ]
    }
   ],
   "source": [
    "data_path = './data/preparation_181_data_Taper_Sum_median_eval.tfrecords'\n",
    "arrays = get_data_from_path(data_path)\n",
    "# example, label = get_data_from_path(data_path)\n",
    "# print(example[0])\n",
    "print((arrays[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex, label = get_data_from_path(data_path)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex, label = get_data_from_path(data_path)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
