{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import time\n",
    "\n",
    "from ImportData2D import get_label, getFilename, get_file_name\n",
    "from stlSlicer import getSlicer, slicecoor, rotatestl\n",
    "from imgSave import saveplot\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(125)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = getLabel(\"Occ_Sum\", \"median\", True, False,False)\n",
    "print(type(label[0]))\n",
    "label = getLabel(\"Occ_Sum\", \"median\", True, True,False)\n",
    "print(type(label[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "test_name = \"prep_test.tfrecords\"\n",
    "test_name = os.path.join(\"data\", test_name)\n",
    "print(test_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "A = './data/preparation_181_data_test_eval_address.txt'\n",
    "print(os.path.abspath(A))\n",
    "print(os.path.isfile(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_grouped_address = ['/home/pasin/Documents/Google_Drive/Aa_TIT_LAB_Comp/Library/Tooth/Tooth/Model/my2DCNN/data/cross_section/PreparationScan_0_84140','/home/pasin/Documents/Google_Drive/Aa_TIT_LAB_Comp/Library/Tooth/Tooth/Model/my2DCNN/data/cross_section/PreparationScan_1_95138_0','/home/pasin/Documents/Google_Drive/Aa_TIT_LAB_Comp/Library/Tooth/Tooth/Model/my2DCNN/data/cross_section/PreparationScan_1_304138_45']\n",
    "grouped_address = [1,2,3]\n",
    "# define an empty list\n",
    "train_address = []\n",
    "\n",
    "# open file and read the content in a list\n",
    "with open(A, 'r') as filehandle:  \n",
    "    for line in filehandle:\n",
    "        # remove linebreak which is the last character of the string\n",
    "        current_name = line[:-1]\n",
    "        # print(current_name)\n",
    "        for i, name in enumerate(example_grouped_address):\n",
    "            \n",
    "            if current_name in name:\n",
    "                train_address.append(grouped_address[i])\n",
    "                grouped_address.remove(grouped_address[i])\n",
    "                example_grouped_address.remove(example_grouped_address[i])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(train_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "B = ast.literal_eval(error_file_names[0])\n",
    "print(B[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_address = ['abc_0_0','abc_0_90','abc_0_45','abc_0_135','abc_1_0','abc_1_45','abc_1_90','abc_1_70']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_address.sort()\n",
    "print([s for s in train_address if 'abc_0' in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import csv\n",
    "A = {'learning_rate':0.00009123,'keep_prob':0.25, 'activation':tf.nn.relu, 'channels':[16,16,32]}\n",
    "# A = {'a':'b','c':'d'}\n",
    "\n",
    "with open(\"output\"+\".csv\", \"w\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for key, val in A.items():\n",
    "        writer.writerow([key, val])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can add the parameter data_home to wherever to where you want to download your data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "((train_data, train_labels),(eval_data, eval_labels)) = tf.keras.datasets.mnist.load_data()\n",
    "train_data = train_data / np.float32(255)\n",
    "train_data = train_data.reshape(train_data.shape[0], -1)\n",
    "print(train_data.shape)\n",
    "train_labels = train_labels.astype(np.int32)  # not required\n",
    "print(train_labels.shape)\n",
    "# test_size: what proportion of original data is used for test set\n",
    "train_img, test_img, train_lbl, test_lbl = train_test_split(\n",
    "    train_data, train_labels, test_size=1/7.0, random_state=0)\n",
    "print(train_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the Data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training set only.\n",
    "scaler.fit(train_img)\n",
    "\n",
    "# Apply transform to both the training set and the test set.\n",
    "train_img = scaler.transform(train_img)\n",
    "test_img = scaler.transform(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "pca = PCA(.95)\n",
    "pca.fit(train_img)\n",
    "pca.n_components_\n",
    "train_img = pca.transform(train_img)\n",
    "test_img = pca.transform(test_img)\n",
    "\n",
    "# all parameters not specified are set to their defaults\n",
    "# default solver is incredibly slow thats why we change it\n",
    "# solver = 'lbfgs'\n",
    "logisticRegr = LogisticRegression(solver = 'lbfgs')\n",
    "\n",
    "logisticRegr.fit(train_img, train_lbl)\n",
    "\n",
    "# Returns a NumPy Array\n",
    "# Predict for One Observation (image)\n",
    "logisticRegr.predict(test_img[0].reshape(1,-1))\n",
    "\n",
    "# Predict for Multiple Observations (images) at Once\n",
    "logisticRegr.predict(test_img[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = logisticRegr.score(test_img, test_lbl)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# tf.enable_eager_execution()\n",
    "import numpy as np\n",
    "\n",
    "# Fixed parameter\n",
    "numdegree = 4  # Number of rotations\n",
    "image_height = 240  # Used for cropping\n",
    "image_width = 360  # Used for cropping\n",
    "\n",
    "\n",
    "# Import tfrecord to dataset\n",
    "def deserialize(example):\n",
    "    feature = {'label': tf.FixedLenFeature([], tf.int64)}\n",
    "    for i in range(numdegree):\n",
    "        feature['img' + str(i)] = tf.FixedLenFeature([], tf.string)\n",
    "    return tf.parse_single_example(example, feature)\n",
    "\n",
    "\n",
    "def decode(data_dict):\n",
    "    if numdegree != 4:\n",
    "        raise Exception('Number of degree specified is not compatible, edit code')\n",
    "    # Create initial image, then stacking it\n",
    "    image_decoded = []\n",
    "\n",
    "    # Stacking the rest\n",
    "    for i in range(0, numdegree):\n",
    "        img = data_dict['img' + str(i)]\n",
    "        file_decoded = tf.image.decode_png(img, channels=1)\n",
    "        file_cropped = tf.squeeze(tf.image.resize_image_with_crop_or_pad(file_decoded, image_height, image_width))\n",
    "        image_decoded.append(file_cropped)\n",
    "\n",
    "    image_stacked = tf.stack([image_decoded[0], image_decoded[1], image_decoded[2], image_decoded[3]], axis=2)\n",
    "    image_stacked = tf.cast(image_stacked, tf.float32)\n",
    "    label = data_dict['label']\n",
    "    # output = (image_stacked, label)\n",
    "    # return {'images': image_stacked, 'label': label}  # Output is [Channel, Height, Width]\n",
    "    return image_stacked, label\n",
    "\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def _float_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def get_data_from_path(data_path):\n",
    "    dataset = tf.data.TFRecordDataset(data_path)\n",
    "    #print(dataset)\n",
    "    dataset = dataset.map(deserialize)\n",
    "    dataset = dataset.map(decode)\n",
    "    \n",
    "    max_elems = np.iinfo(np.int64).max\n",
    "    dataset = dataset.batch(1000, drop_remainder=False)\n",
    "    whole_dataset_tensors = tf.data.experimental.get_single_element(dataset)\n",
    "    with tf.Session() as sess:\n",
    "        whole_dataset_arrays = sess.run(whole_dataset_tensors)\n",
    "    return whole_dataset_arrays\n",
    "    \n",
    "    '''\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    next_example, next_label = iterator.get_next()\n",
    "    with tf.Session() as sess:\n",
    "        example = sess.run(next_example)\n",
    "        label = sess.run(next_label)\n",
    "    return example, label\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/preparation_181_data_Taper_Sum_median_eval.tfrecords'\n",
    "arrays = get_data_from_path(data_path)\n",
    "# example, label = get_data_from_path(data_path)\n",
    "# print(example[0])\n",
    "print((arrays[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex, label = get_data_from_path(data_path)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex, label = get_data_from_path(data_path)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "csvData = [['Person', 'Age'], ['Peter', '22'], ['Jasmine', '21'], ['Sam', '24']]\n",
    "print(type(csvData))\n",
    "print((csvData))\n",
    "print(type(csvData[0]))\n",
    "print((csvData[0]))\n",
    "'''\n",
    "with open('person.csv', 'w') as csvFile:\n",
    "    writer = csv.writer(csvFile)\n",
    "    writer.writerows(csvData)\n",
    "\n",
    "csvFile.close()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvData = [(-0.53103447, [1e-05, 0.0, '1', 1]), (-0.53103447, [4.315202424267864e-05, 0.0, '1', 1]), (-0.53103447, [6.388961292348475e-05, 0.04720173848853115, '1', 4]), (-0.53103447, [0.0001540877645803547, 0.35419707255064065, '1', 1])]\n",
    "newData = []\n",
    "for i in csvData:\n",
    "    data = {'value': i[0],\n",
    "           'learning_rate': i[1][0],\n",
    "           'dropout_rate': i[1][1],\n",
    "           'activation': i[1][1],\n",
    "           'channels': i[1][1],}\n",
    "    newData.append(data)\n",
    "print(type(csvData))\n",
    "print((csvData))\n",
    "#print(type(csvData[0]))\n",
    "# print((csvData[0]))\n",
    "\n",
    "\n",
    "with open('person.csv', 'w') as csvFile:\n",
    "    writer = csv.DictWriter(csvFile, fieldnames = ['value','learning_rate','dropout_rate', 'activation', 'channels'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(newData)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# tf.enable_eager_execution()\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "from shutil import copy2\n",
    "import csv\n",
    "import datetime\n",
    "\n",
    "from protobuf_helper import protobuf_to_list, protobuf_to_channels\n",
    "from proto import tooth_pb2\n",
    "from google.protobuf import text_format\n",
    "\n",
    "import skopt\n",
    "from skopt import gp_minimize, forest_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "from cifar10_model import my_model\n",
    "from cifar10_get_data import train_input_fn, eval_input_fn, get_data_from_path\n",
    "\n",
    "activation_dict = {'0': tf.nn.relu, '1': tf.nn.leaky_relu}  # Declare global dictionary\n",
    "\n",
    "# These are important parameters\n",
    "run_params = {'batch_size': 4,\n",
    "              'checkpoint_min': 5,\n",
    "              'early_stop_step': 10000,\n",
    "              'input_path': './data/cifar10.tfrecords',\n",
    "              'result_path': '/home/pasin/Documents/Pasin/model/cifar10_hyper',\n",
    "              'config_path': '',\n",
    "              'steps': 100}\n",
    "\n",
    "model_configs = {'learning_rate': 0.0001,\n",
    "                 'dropout_rate': 0.1,\n",
    "                 'activation': tf.nn.relu,\n",
    "                 'channels': [16, 16, 32, 32, 32, 32, 32, 32, 32, 2048, 2048]\n",
    "                 }\n",
    "\n",
    "\n",
    "def run(model_params={}):\n",
    "    # Add exception in case params is missing\n",
    "    if len(model_params['channels']) != 11:\n",
    "        raise Exception(\"Number of channels not correspond to number of layers [Need size of 11, got %s]\"\n",
    "                        % len(model_params['channels']))\n",
    "\n",
    "    # Type in file name\n",
    "    train_data_path = run_params['input_path'].replace('.tfrecords', '') + '_train.tfrecords'\n",
    "    eval_data_path = run_params['input_path'].replace('.tfrecords', '') + '_eval.tfrecords'\n",
    "    print(\"Getting training data from %s\" % train_data_path)\n",
    "    print(\"Saved model at %s\" % run_params['result_path_new'])\n",
    "\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)  # To see some additional info\n",
    "    # Setting checkpoint config\n",
    "    my_checkpoint_config = tf.estimator.RunConfig(\n",
    "        save_checkpoints_secs=run_params['checkpoint_min'] * 60,\n",
    "        # save_summary_steps=pareval_data_pathams['checkpoint_min'] * 10,\n",
    "        keep_checkpoint_max=10,\n",
    "        log_step_count_steps=500,\n",
    "        session_config=tf.ConfigProto(allow_soft_placement=True)\n",
    "    )\n",
    "    # Or set up the model directory\n",
    "    #   estimator = DNNClassifier(\n",
    "    #       config=tf.estimator.RunConfig(\n",
    "    #           model_dir='/my_model', save_summary_steps=100),\n",
    "    classifier = tf.estimator.Estimator(\n",
    "        model_fn=my_model,\n",
    "        params=model_params,\n",
    "        model_dir=run_params['result_path_new'],\n",
    "        config=my_checkpoint_config\n",
    "    )\n",
    "    train_hook = tf.contrib.estimator.stop_if_no_decrease_hook(classifier, \"loss\", run_params['early_stop_step'])\n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn=lambda: train_input_fn(train_data_path, batch_size=run_params['batch_size']),\n",
    "        max_steps=run_params['steps'], hooks=[train_hook])\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn=lambda: eval_input_fn(eval_data_path, batch_size=16), steps=None,\n",
    "        start_delay_secs=0, throttle_secs=0)\n",
    "    # classifier.train(input_fn=lambda: train_input_fn(train_data_path, batch_size=params['batch_size']),\n",
    "    #     max_steps=params['steps'], hooks=[train_hook])\n",
    "    # eval_result = classifier.evaluate(input_fn=lambda: eval_input_fn(eval_data_path, batch_size=32))\n",
    "\n",
    "    eval_result = tf.estimator.train_and_evaluate(classifier, train_spec, eval_spec)\n",
    "    print(\"Eval result:\")\n",
    "    print(eval_result)\n",
    "    try:\n",
    "        accuracy = eval_result[0]['accuracy']\n",
    "        global_step = eval_result[0]['global_step']\n",
    "    except TypeError:\n",
    "        print(\"Warning, does receive evaluation result\")\n",
    "        accuracy = 0\n",
    "        global_step = 0\n",
    "\n",
    "    predictions = classifier.predict(input_fn=lambda: eval_input_fn(eval_data_path, batch_size=1))\n",
    "\n",
    "    images, expected = get_data_from_path(eval_data_path)\n",
    "    predict_score = ['Prediction']\n",
    "    probability_score = ['Probability']\n",
    "    label_score = ['Label']\n",
    "    for pred_dict, expec in zip(predictions, expected):\n",
    "        # print(pred_dict)\n",
    "        print(\"Score\" + str(pred_dict['score']))\n",
    "        class_id = pred_dict['score'][0]\n",
    "        probability = pred_dict['probabilities'][class_id]\n",
    "        print(\"Actual score: %s, Predicted score: %s with probability %s\" % (expec, class_id, probability))\n",
    "        predict_score.append(class_id)\n",
    "        label_score.append(expec)\n",
    "        probability_score.append(probability)\n",
    "\n",
    "    # predict_result = zip(label_score, predict_score, probability_score)\n",
    "\n",
    "    predict_result = zip(label_score, predict_score, probability_score)\n",
    "    # print(eval_result[0]['accuracy'])\n",
    "    # print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))\n",
    "    return accuracy, global_step, predict_result\n",
    "\n",
    "\n",
    "dim_learning_rate = Real(low=1e-5, high=1e-2, prior='log-uniform', name='learning_rate')\n",
    "dim_dropout_rate = Real(low=0, high=0.875, name='dropout_rate')\n",
    "dim_activation = Categorical(categories=['0', '1'],\n",
    "                             name='activation')\n",
    "dim_channel = Integer(low=1, high=4, name='channels')\n",
    "dimensions = [dim_learning_rate,\n",
    "              dim_dropout_rate,\n",
    "              dim_activation,\n",
    "              dim_channel]\n",
    "default_parameters = [1e-3, 0.125, '1', 2]\n",
    "\n",
    "\n",
    "@use_named_args(dimensions=dimensions)\n",
    "def fitness(learning_rate, dropout_rate, activation, channels):\n",
    "    \"\"\"\n",
    "    Hyper-parameters:\n",
    "    learning_rate:     Learning-rate for the optimizer.\n",
    "    dropout_rate:\n",
    "    activation:        Activation function for all layers.\n",
    "    channels\n",
    "    \"\"\"\n",
    "    # Create the neural network with these hyper-parameters\n",
    "    print(\"Learning_rate, Dropout_rate, Activation, Channels = %s, %s, %s, %s\" % (\n",
    "        learning_rate, dropout_rate, activation, channels))\n",
    "    channels_full = [i * channels for i in [16, 16, 32, 16, 16, 16, 16, 16, 16, 512, 512]]\n",
    "    name = run_params['result_path'] + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d_%H_%M_%S\") + \"/\"\n",
    "    # name = (\"%s/learning_rate_%s_dropout_%s_activation_%s_channels_%s/\"\n",
    "    #         % (run_params['result_path'], round(learning_rate, 6), dropout_rate, activation, channels))\n",
    "    run_params['result_path_new'] = name\n",
    "    md_config = {'learning_rate': learning_rate,\n",
    "                 'dropout_rate': dropout_rate,\n",
    "                 'activation': activation_dict[activation],\n",
    "                 'channels': channels_full}\n",
    "    accuracy, global_step, result = run(md_config)\n",
    "    # accuracy = run_hyper_parameter_wrapper(learning_rate, dropout_rate, activation, channels)\n",
    "\n",
    "    # Save necessary info to csv file, as reference\n",
    "    info_dict = run_params.copy()\n",
    "    info_dict['comments'] = \"Try hyper search on cifar10\"\n",
    "    info_dict['learning_rate'] = learning_rate\n",
    "    info_dict['dropout_rate'] = dropout_rate\n",
    "    info_dict['activation'] = activation\n",
    "    info_dict['channels'] = channels\n",
    "    info_dict['steps'] = global_step\n",
    "    info_dict['accuracy'] = accuracy\n",
    "    with open(name + \"config.csv\", \"w\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for key, val in info_dict.items():\n",
    "            writer.writerow([key, val])\n",
    "    with open(name + \"result.csv\", \"w\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for row in result:\n",
    "            writer.writerow(row)\n",
    "    return -accuracy\n",
    "\n",
    "\n",
    "search_result = gp_minimize(func=fitness,\n",
    "                                dimensions=dimensions,\n",
    "                                acq_func='EI',  # Expected Improvement.\n",
    "                                n_calls=11,\n",
    "                                x0=default_parameters)\n",
    "print(search_result)\n",
    "print(\"Best hyper-parameters: %s\" % search_result.x)\n",
    "searched_parameter = sorted(list(zip(search_result.func_vals, search_result.x_iters)))\n",
    "print(\"All hyper-parameter searched: %s\" % searched_parameter)\n",
    "hyperparameter_filename = \"hyperparameters_\" + datetime.datetime.now().strftime(\"%Y%m%d_%H_%M_%S\") + \".csv\"\n",
    "new_data = []\n",
    "field_name = ['accuracy', 'learning_rate', 'dropout_rate', 'activation', 'channels']\n",
    "print(searched_parameter)\n",
    "for i in searched_parameter:\n",
    "    data = {field_name[0]: i[0] * -1,\n",
    "                field_name[1]: i[1][0],\n",
    "                field_name[2]: i[1][1],\n",
    "                field_name[3]: i[1][1],\n",
    "                field_name[4]: i[1][1]}\n",
    "    new_data.append(data)\n",
    "with open(run_params['result_path'] + '/' + hyperparameter_filename, 'w', newline='') as csvFile:\n",
    "    writer = csv.DictWriter(csvFile, fieldnames=field_name)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(new_data)\n",
    "# space = search_result.space\n",
    "# print(\"Best result: %s\" % space.point_to_dict(search_result.x))\n",
    "\n",
    "print(\"train.py completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Func vals: %s \\n\" % search_result.func_vals)\n",
    "print(\"Hyper para: %s \\n\" % search_result.x_iters)\n",
    "print(\"Combined: %s\" % list(zip(search_result.func_vals, search_result.x_iters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searched_parameter_2 = (list(zip(search_result.func_vals, search_result.x_iters)))\n",
    "i = searched_parameter_2[0]\n",
    "data_2 = {field_name[0]: i[0] * -1,\n",
    "                field_name[1]: i[1][0],\n",
    "                field_name[2]: i[1][1],\n",
    "                field_name[3]: i[1][1],\n",
    "                field_name[4]: i[1][1]}\n",
    "print(data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImportData2D.py version: 2.3.1\n",
      "stlSlicer.py version: 1.8.1\n",
      "pre_processing.py version: 1.2.0\n",
      "get_file_name: Import from /home/pasin/Documents/Google_Drive/Aa_TIT_LAB_Comp/Library/Tooth/Tooth/Model/global_data, searching for file name with PreparationScan.stl\n",
      "get_file_name: Uploaded 365 file names\n",
      "get_label: Import from ../global_data/Ground Truth Score_new.csv\n",
      "get_label: Upload non one-hotted label completed (as a list): 730 examples\n",
      "getSlicer: /home/pasin/Documents/Google_Drive/Aa_TIT_LAB_Comp/Library/Tooth/Tooth/Model/global_data/Original Format Data/304121-2/PreparationScan.stl has problem getting cross-section. Possible hole appeared in model\n",
      "getSlicer: /home/pasin/Documents/Google_Drive/Aa_TIT_LAB_Comp/Library/Tooth/Tooth/Model/global_data/Original Format Data/84119-2/PreparationScan.stl has problem getting cross-section. Possible hole appeared in model\n",
      "getSlicer: /home/pasin/Documents/Google_Drive/Aa_TIT_LAB_Comp/Library/Tooth/Tooth/Model/global_data/Original Format Data/94106-2/PreparationScan.stl has problem getting cross-section. Possible hole appeared in model\n",
      "Finished with 362 examples, 362 augmented examples\n",
      "Number of score received: 724 (Originally: 362\n",
      "Max point: 367, min  point: 231\n",
      "<class 'list'>\n",
      "362\n",
      "<class 'list'>\n",
      "4\n",
      "<class 'numpy.ndarray'>\n",
      "(353, 2)\n",
      "pre_processing.py: done\n"
     ]
    }
   ],
   "source": [
    "# This file is to convert stl file and csv file into cross_section image\n",
    "# After running this, use imgtotfrecord.py to turn images into tfrecord\n",
    "\n",
    "# Import Libraries\n",
    "import time\n",
    "from ImportData2D import get_label, get_file_name, save_plot\n",
    "from stlSlicer import getSlicer, slicecoor, rotatestl\n",
    "import numpy as np\n",
    "\n",
    "v = '1.2.0'\n",
    "# Initial version: Based on main.ipynb\n",
    "# 1.1: Implemented ignore data that has problem\n",
    "# 1.2: Now save image with their own name\n",
    "print(\"pre_processing.py version: \" + str(v))\n",
    "\n",
    "degree = list([0, 45, 90, 135])\n",
    "\n",
    "def main():\n",
    "    # Get data and transformed to cross-section image\n",
    "    name_dir, image_name = get_file_name(folder_name='../global_data/', file_name=\"PreparationScan.stl\")\n",
    "    label, label_name = get_label(\"Taper_Sum\", \"median\", double_data=True, one_hotted=False, normalized=False)\n",
    "    # Number of data should be the same as number of label\n",
    "    if image_name != label_name:\n",
    "        print(image_name)\n",
    "        print(label_name)\n",
    "        diff = list(set(image_name).symmetric_difference(set(label_name)))\n",
    "        raise Exception(\"ERROR, image and label not similar: %d images, %d labels. Possible missing files: %s\"\n",
    "                        % (len(image_name), (len(label_name)), diff))\n",
    "\n",
    "    augment_config = list([False, True])  # Original data once, Augmented once\n",
    "    # Prepare two set of list, one for data, another for augmented data\n",
    "    stl_points = list()\n",
    "    stl_points_augmented = list()\n",
    "    error_file_names = list()  # Names of file that cannot get cross-section image\n",
    "    min_point = 1000\n",
    "    max_point = 0\n",
    "    for i in range(len(name_dir)):\n",
    "        for augment in augment_config:\n",
    "            points = getSlicer(name_dir[i], 0, degree, augment, axis=1)\n",
    "            if points is None:  # If the output has error, remove label of that file\n",
    "                error_file_names.append(image_name[i])\n",
    "                index = label_name.index(image_name[i])\n",
    "                label_name.pop(index)\n",
    "                label.pop(index * 2)\n",
    "                label.pop(index * 2)  # Do it again if we double the data\n",
    "                break\n",
    "            else:\n",
    "                if len(points[0]) > max_point:\n",
    "                    max_point = len(points[0])\n",
    "                if len(points[0]) < min_point:\n",
    "                    min_point = len(points[0])\n",
    "                if augment:\n",
    "                    stl_points_augmented.append(points)\n",
    "                else:\n",
    "                    stl_points.append(points)\n",
    "\n",
    "    # The output is list(examples) of list(degrees) of numpy array (N*2 coordinates)\n",
    "    print(\"Finished with %d examples, %d augmented examples\" % (len(stl_points), len(stl_points_augmented)))\n",
    "    print(\"Number of score received: %d (Originally: %d\" % (len(label), len(label) / 2))\n",
    "\n",
    "    print(\"Max point: %s, min  point: %s\" % (max_point, min_point))\n",
    "    # augment_num = int(len(label)/len(label_name))\n",
    "    # label_name_aug = [val for val in label_name for _ in range(augment_num)]\n",
    "    return stl_points, stl_points_augmented, label_name, error_file_names, degree\n",
    "\n",
    "\n",
    "def save_image(stl_points, stl_points_augmented, label_name, error_file_names):\n",
    "    # Save data as png image\n",
    "    png_name = \"PreparationScan\" + \"_0\"\n",
    "    save_plot(stl_points, \"./data/cross_section\", png_name, label_name, degree)\n",
    "    print(\"Finished saving first set of data\")\n",
    "    # Save again for augmented data\n",
    "    png_name = \"PreparationScan\" + \"_1\"\n",
    "    save_plot(stl_points_augmented, \"./data/cross_section\", png_name, label_name, degree)\n",
    "    print(\"Finished saving second set of data\")\n",
    "\n",
    "    # Save names which has defect on it, use when convert to tfrecord\n",
    "    with open('./data/cross_section/error_file.txt', 'w') as filehandle:\n",
    "        for listitem in error_file_names:\n",
    "            filehandle.write('%s\\n' % listitem)\n",
    "\n",
    "\n",
    "def save_movement(stl_points, label_name):\n",
    "    new_stl_points = []\n",
    "    for stl_point_sample in stl_points:\n",
    "        new_points_sample = []\n",
    "        for d in range(len(degree)):\n",
    "            points = stl_point_sample[0]\n",
    "        new_stl_points.append(new_points_sample)\n",
    "\n",
    "\n",
    "\n",
    "points, points_aug, lbl_name, err_name, deg = main()  # Output points as list[list[numpy]] (example_data, degrees, points)\n",
    "print(type(points))\n",
    "print(len(points))\n",
    "print(type(points[0]))\n",
    "print(len(points[0]))\n",
    "print(type(points[0][0]))\n",
    "print(np.shape(points[0][0]))\n",
    "# save_image(points, points_aug, lbl_name, err_name)\n",
    "print(\"pre_processing.py: done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stl_points = points\n",
    "new_stl_points = []\n",
    "for stl_point_sample in stl_points:  # stl_point_sample is one example of stl_points\n",
    "    new_points_sample = []\n",
    "    for stl_point_image in stl_point_sample:  # stl_point_image is one degree of cross-section\n",
    "        difference = stl_point_image[1:, :] - stl_point_image[0:-1, :]  # Find difference between each position\n",
    "        new_points_sample.append(difference)\n",
    "    new_stl_points.append(new_points_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "362\n",
      "<class 'list'>\n",
      "4\n",
      "<class 'numpy.ndarray'>\n",
      "(352, 2)\n"
     ]
    }
   ],
   "source": [
    "print(type(new_stl_points))\n",
    "print(len(new_stl_points))\n",
    "print(type(new_stl_points[0]))\n",
    "print(len(new_stl_points[0]))\n",
    "print(type(new_stl_points[0][0]))\n",
    "print(np.shape(new_stl_points[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-6.7410000e-05  3.0381200e-03]\n",
      " [-7.9602000e-04  3.5868770e-02]\n",
      " [-1.2950000e-03  1.0284902e-01]\n",
      " [-2.9505600e-03  5.3032030e-02]\n",
      " [-5.2922700e-03  1.2944522e-01]\n",
      " [-1.7759400e-03  7.7626380e-02]\n",
      " [-1.5305900e-03  7.1442220e-02]\n",
      " [-4.0421400e-03  1.6210612e-01]\n",
      " [-4.2452600e-03  6.7096730e-02]\n",
      " [-2.7566200e-03  1.1339957e-01]\n",
      " [-6.7806600e-03  1.4826855e-01]\n",
      " [-5.2958100e-03  9.5722900e-02]\n",
      " [-4.2223400e-03  1.2270431e-01]\n",
      " [-3.5535000e-04  2.5624560e-02]\n",
      " [-1.0479700e-03  2.6030330e-02]\n",
      " [-2.1326300e-03  1.1554921e-01]\n",
      " [-1.3645100e-03  4.6904350e-02]\n",
      " [-5.6744300e-03  1.0090443e-01]\n",
      " [-7.6224000e-04  1.4843120e-02]\n",
      " [-1.3419000e-04  4.4982600e-03]\n",
      " [-8.7140000e-05  3.7094100e-03]\n",
      " [-9.2631600e-03  1.4819503e-01]\n",
      " [-1.6463100e-03  5.2617150e-02]\n",
      " [-4.7271400e-03  1.4477225e-01]\n",
      " [-3.6569300e-03  7.7267230e-02]\n",
      " [-9.9820000e-04  2.7121810e-02]\n",
      " [-2.1981000e-03  3.0852160e-02]\n",
      " [-8.4874100e-03  8.9498350e-02]\n",
      " [-2.6326800e-03  3.2619140e-02]\n",
      " [-9.7921600e-03  9.3374320e-02]\n",
      " [-5.6844600e-03  5.5465850e-02]\n",
      " [-3.1864000e-03  4.3125170e-02]\n",
      " [-1.8834500e-03  4.3948560e-02]\n",
      " [-5.5041600e-03  1.3734753e-01]\n",
      " [-9.5039000e-03  1.4199986e-01]\n",
      " [-3.6919400e-03  4.6751940e-02]\n",
      " [-4.5658200e-03  4.4169050e-02]\n",
      " [-1.1525260e-02  1.0284199e-01]\n",
      " [-2.6317400e-03  2.5742840e-02]\n",
      " [-1.6000000e-03  1.8274260e-02]\n",
      " [-5.6790000e-03  1.2467435e-01]\n",
      " [-8.5869300e-03  6.0269380e-02]\n",
      " [-3.1844300e-03  2.6690500e-02]\n",
      " [-6.9299300e-03  3.5016980e-02]\n",
      " [-1.9557640e-02  8.3556630e-02]\n",
      " [-1.1725440e-02  4.1263280e-02]\n",
      " [-5.3230000e-03  2.7155790e-02]\n",
      " [-3.1809100e-02  1.2129869e-01]\n",
      " [-4.9426000e-03  2.0572440e-02]\n",
      " [-4.8965200e-03  1.0355940e-02]\n",
      " [-2.1701050e-02  5.6509600e-02]\n",
      " [-3.1530750e-02  6.6413080e-02]\n",
      " [-1.8472730e-02  3.3678350e-02]\n",
      " [-4.9420070e-02  9.9368590e-02]\n",
      " [-2.4070340e-02  4.9302020e-02]\n",
      " [-1.0553120e-02  2.8276980e-02]\n",
      " [-1.6243640e-02  4.7953190e-02]\n",
      " [-7.6952200e-03  3.5718860e-02]\n",
      " [-1.0216630e-02  5.9187330e-02]\n",
      " [-7.6472400e-03  6.3556900e-02]\n",
      " [-1.5496590e-02  8.1019310e-02]\n",
      " [-3.2195000e-04  1.4009200e-03]\n",
      " [-7.9225000e-04  2.8103200e-03]\n",
      " [-6.8570900e-03  2.5157660e-02]\n",
      " [-1.3182970e-02  4.9010680e-02]\n",
      " [-2.3116830e-02  8.0024540e-02]\n",
      " [-2.8351720e-02  1.3123736e-01]\n",
      " [-1.0903110e-02  5.0867930e-02]\n",
      " [-4.5214670e-02  1.8861664e-01]\n",
      " [-1.6200410e-02  6.2672600e-02]\n",
      " [-6.0422280e-02  2.0458253e-01]\n",
      " [-9.3764000e-04  3.8763300e-03]\n",
      " [-4.6246000e-04  2.8657500e-03]\n",
      " [-6.3901500e-03  7.7417060e-02]\n",
      " [ 2.5431400e-03  2.0869810e-02]\n",
      " [ 6.7368200e-03  3.0810120e-02]\n",
      " [ 7.5810600e-03  2.6720960e-02]\n",
      " [ 1.5455200e-03  1.1670600e-03]\n",
      " [ 4.2821480e-02  2.2639930e-02]\n",
      " [ 2.8447340e-02  2.0601250e-02]\n",
      " [ 2.3561440e-02 -7.8527000e-04]\n",
      " [ 3.2197740e-02 -3.2881600e-03]\n",
      " [ 1.0578708e-01 -1.4573330e-02]\n",
      " [ 3.4736840e-02  4.4590000e-05]\n",
      " [ 4.7820290e-02 -3.8389600e-03]\n",
      " [ 6.1834320e-02  7.3865100e-03]\n",
      " [ 1.2776600e-02  9.7306000e-04]\n",
      " [ 1.1771590e-02  2.0883700e-03]\n",
      " [ 6.8602820e-02  1.2132520e-02]\n",
      " [ 2.4176000e-02  6.8444400e-03]\n",
      " [ 5.1933150e-02  1.2685820e-02]\n",
      " [ 5.6917960e-02  2.0950260e-02]\n",
      " [ 3.4860610e-02  1.2013290e-02]\n",
      " [ 2.2391630e-02  1.0129280e-02]\n",
      " [ 5.4092340e-02  2.5976330e-02]\n",
      " [ 4.9244200e-02  2.6839520e-02]\n",
      " [ 2.7436690e-02  1.6039740e-02]\n",
      " [ 6.3270700e-02  4.9820110e-02]\n",
      " [ 1.7280350e-02  1.2336860e-02]\n",
      " [ 3.8839600e-02  4.0063760e-02]\n",
      " [ 3.6074350e-02  3.7443450e-02]\n",
      " [ 7.2466600e-03  8.5307900e-03]\n",
      " [ 4.1093890e-02  5.9223940e-02]\n",
      " [ 2.0586930e-02  2.7650650e-02]\n",
      " [ 1.2194810e-02  1.8406490e-02]\n",
      " [ 3.5323960e-02  6.0435440e-02]\n",
      " [ 2.2177890e-02  4.8381870e-02]\n",
      " [ 1.7657450e-02  3.0306320e-02]\n",
      " [ 1.4805190e-02  4.5239380e-02]\n",
      " [ 7.3201100e-03  1.9263560e-02]\n",
      " [ 1.8305200e-03  7.0750100e-03]\n",
      " [ 2.2608500e-03  1.6558050e-02]\n",
      " [ 1.3443710e-02  7.1352430e-02]\n",
      " [ 1.6592900e-03  8.4669700e-02]\n",
      " [ 4.1748800e-03  8.5466740e-02]\n",
      " [ 2.8261700e-03  6.4879550e-02]\n",
      " [-6.3894000e-04  5.0543050e-02]\n",
      " [ 1.2876900e-03  3.4314780e-02]\n",
      " [-8.2410000e-05  1.1288400e-02]\n",
      " [-1.2018000e-03  1.8162840e-02]\n",
      " [-3.5272800e-03  3.4201540e-02]\n",
      " [-1.7334640e-02  1.9663482e-01]\n",
      " [-4.8001000e-03  6.9620430e-02]\n",
      " [-2.3741000e-03  1.0836513e-01]\n",
      " [-7.8879000e-04  3.9917720e-02]\n",
      " [-3.1160600e-03  5.3071200e-02]\n",
      " [ 1.4921300e-03  8.8398720e-02]\n",
      " [-2.3508000e-04  6.4145430e-02]\n",
      " [ 9.5786700e-03  1.4430317e-01]\n",
      " [ 1.3237300e-03  2.4158580e-02]\n",
      " [ 1.1819400e-03  2.4188070e-02]\n",
      " [ 7.8537000e-03  9.2249230e-02]\n",
      " [ 2.1131200e-03  5.2023040e-02]\n",
      " [ 5.8692400e-03  8.0872790e-02]\n",
      " [ 1.0179000e-04  1.6770880e-02]\n",
      " [ 5.0221000e-03  4.0600460e-02]\n",
      " [ 5.6627600e-03  3.4503070e-02]\n",
      " [ 1.9481550e-02  4.8789060e-02]\n",
      " [ 1.2584330e-02  3.6913920e-02]\n",
      " [ 1.7496150e-02  2.3985590e-02]\n",
      " [ 4.4274440e-02  5.7065120e-02]\n",
      " [ 5.6281530e-02  5.5561600e-02]\n",
      " [ 4.2035240e-02  3.9060170e-02]\n",
      " [ 6.8120700e-02  5.1905320e-02]\n",
      " [ 1.0245060e-02  8.1846100e-03]\n",
      " [ 2.8601210e-02  1.1747510e-02]\n",
      " [ 5.0847860e-02  2.1550670e-02]\n",
      " [ 7.7884800e-03  2.1320400e-03]\n",
      " [ 1.3318070e-02  3.0545700e-03]\n",
      " [ 1.0705836e-01  3.0276270e-02]\n",
      " [ 3.7121240e-02  8.4056100e-03]\n",
      " [ 7.0938390e-02  2.0550250e-02]\n",
      " [ 5.6510860e-02  2.0904230e-02]\n",
      " [ 3.3672170e-02  1.5003910e-02]\n",
      " [ 1.7514810e-02  1.0697960e-02]\n",
      " [ 3.1617740e-02  1.5020890e-02]\n",
      " [ 1.0435308e-01  5.3738100e-02]\n",
      " [ 6.6931510e-02  2.7841590e-02]\n",
      " [ 5.9277800e-03  2.4273200e-03]\n",
      " [ 7.5469130e-02  2.4356230e-02]\n",
      " [ 1.7582430e-02  4.5522200e-03]\n",
      " [ 1.1207457e-01  2.6308600e-02]\n",
      " [ 4.2502360e-02  7.1323600e-03]\n",
      " [ 4.8647860e-02  4.3167300e-03]\n",
      " [ 2.7776580e-02  3.3269500e-03]\n",
      " [ 2.7288480e-02  5.7352000e-04]\n",
      " [ 8.4202010e-02 -5.9140000e-04]\n",
      " [ 1.5539045e-01 -9.7762800e-03]\n",
      " [ 2.6583430e-02 -7.3430000e-04]\n",
      " [ 7.4400800e-03 -3.2215000e-04]\n",
      " [ 6.8246800e-03 -5.1910000e-05]\n",
      " [ 1.2560704e-01  3.4590000e-04]\n",
      " [ 1.4257220e-02 -5.9132000e-04]\n",
      " [ 1.1447916e-01 -8.4642500e-03]\n",
      " [ 2.8580640e-02 -3.7647000e-03]\n",
      " [ 1.3757083e-01 -1.2708770e-02]\n",
      " [ 4.0741500e-03 -2.7983000e-04]\n",
      " [ 1.6585700e-03 -1.0786000e-04]\n",
      " [ 1.5610200e-03 -1.5760000e-05]\n",
      " [ 6.7732140e-02 -2.1065700e-03]\n",
      " [ 3.8654000e-04  1.8110000e-05]\n",
      " [ 8.4802570e-02  7.8500300e-03]\n",
      " [ 1.1353874e-01  1.5542260e-02]\n",
      " [ 3.2337100e-02  5.7420000e-03]\n",
      " [ 9.4377200e-03  7.5450000e-04]\n",
      " [ 8.5427830e-02  9.6332600e-03]\n",
      " [ 9.0686300e-03 -8.8800000e-06]\n",
      " [ 1.4093652e-01  6.0211300e-03]\n",
      " [ 1.2661350e-02 -6.7514000e-04]\n",
      " [ 9.5115200e-02 -1.0031400e-02]\n",
      " [ 2.9686150e-02 -5.6346000e-03]\n",
      " [ 9.2960370e-02 -1.2903970e-02]\n",
      " [ 2.1323250e-02 -4.3772600e-03]\n",
      " [ 5.3051460e-02 -9.5462300e-03]\n",
      " [ 2.3304580e-02 -3.8596400e-03]\n",
      " [ 4.6888040e-02 -1.4632720e-02]\n",
      " [ 3.2227040e-02 -5.9624000e-03]\n",
      " [ 3.3221330e-02 -1.2510630e-02]\n",
      " [ 3.9273100e-02 -1.0377150e-02]\n",
      " [ 4.2054460e-02 -1.5741710e-02]\n",
      " [ 3.9099430e-02 -1.0084580e-02]\n",
      " [ 9.0025640e-02 -3.4146520e-02]\n",
      " [ 3.1466730e-02 -1.0258580e-02]\n",
      " [ 2.3050140e-02 -7.3496400e-03]\n",
      " [ 6.0577010e-02 -8.6004900e-03]\n",
      " [ 4.6693630e-02 -5.8663200e-03]\n",
      " [ 2.4605730e-02 -2.0360100e-03]\n",
      " [ 4.7501990e-02 -5.4361600e-03]\n",
      " [ 4.5801020e-02 -8.1651900e-03]\n",
      " [ 3.0757270e-02 -6.8828200e-03]\n",
      " [ 7.0440660e-02 -2.1260480e-02]\n",
      " [ 2.3812060e-02 -8.2152200e-03]\n",
      " [ 2.2749380e-02 -1.3157940e-02]\n",
      " [ 1.5336120e-02 -8.5701100e-03]\n",
      " [ 9.9545380e-02 -6.3458020e-02]\n",
      " [ 7.5734050e-02 -5.1535990e-02]\n",
      " [ 9.0235060e-02 -5.7304310e-02]\n",
      " [ 5.4077140e-02 -4.4196580e-02]\n",
      " [ 2.0734870e-02 -1.7997770e-02]\n",
      " [ 5.2061400e-03 -6.0377300e-03]\n",
      " [ 3.6712360e-02 -5.7593050e-02]\n",
      " [ 4.7639400e-03 -8.8516100e-03]\n",
      " [ 6.8958800e-03 -2.2823170e-02]\n",
      " [ 1.9472710e-02 -5.9101360e-02]\n",
      " [ 1.2472840e-02 -7.3750740e-02]\n",
      " [ 6.5690600e-03 -4.4041040e-02]\n",
      " [ 1.2380920e-02 -1.3072351e-01]\n",
      " [ 1.1632260e-02 -7.8905940e-02]\n",
      " [ 9.5142000e-03 -5.5040410e-02]\n",
      " [ 7.1188200e-03 -4.6619870e-02]\n",
      " [ 9.4031400e-03 -7.8210860e-02]\n",
      " [ 4.2036600e-03 -3.7068290e-02]\n",
      " [ 3.3308500e-03 -3.8469780e-02]\n",
      " [ 2.9227300e-03 -8.2072560e-02]\n",
      " [ 2.3055700e-03 -6.9229780e-02]\n",
      " [ 1.8430200e-03 -4.0048350e-02]\n",
      " [ 2.1408800e-03 -3.3398060e-02]\n",
      " [ 9.7991700e-03 -8.7311760e-02]\n",
      " [ 5.5561400e-03 -8.3759570e-02]\n",
      " [ 1.3556500e-03 -1.7995420e-02]\n",
      " [ 1.4900400e-03 -3.1542050e-02]\n",
      " [ 5.2242300e-03 -1.5118569e-01]\n",
      " [ 3.5409300e-03 -7.5929830e-02]\n",
      " [ 8.8748000e-04 -6.0541870e-02]\n",
      " [ 7.3488000e-04 -8.1562300e-02]\n",
      " [ 2.2428400e-03 -7.6188340e-02]\n",
      " [ 1.8365900e-03 -5.7037160e-02]\n",
      " [ 3.4852300e-03 -4.9325610e-02]\n",
      " [ 5.9101800e-03 -4.0354450e-02]\n",
      " [ 1.0018740e-02 -5.6540500e-02]\n",
      " [ 7.6785300e-03 -3.2004160e-02]\n",
      " [ 1.6770680e-02 -5.2859170e-02]\n",
      " [ 1.6095960e-02 -4.3390440e-02]\n",
      " [ 1.2174670e-02 -2.3897550e-02]\n",
      " [ 5.3391030e-02 -9.0683940e-02]\n",
      " [ 6.4226800e-03 -1.0808950e-02]\n",
      " [ 1.9979200e-02 -2.5002250e-02]\n",
      " [ 3.8849070e-02 -4.6457360e-02]\n",
      " [ 2.8359290e-02 -2.5169960e-02]\n",
      " [ 5.5613370e-02 -3.5477760e-02]\n",
      " [ 2.4869020e-02 -1.5518780e-02]\n",
      " [ 2.3966300e-02 -1.0272010e-02]\n",
      " [ 5.1643390e-02 -2.0672250e-02]\n",
      " [ 2.8690650e-02 -9.4672200e-03]\n",
      " [ 7.0314260e-02 -1.7193240e-02]\n",
      " [ 2.7680180e-02 -5.7630300e-03]\n",
      " [ 3.8045030e-02 -5.6768400e-03]\n",
      " [ 9.8053700e-02 -9.9978900e-03]\n",
      " [ 2.4294260e-02 -4.2290400e-03]\n",
      " [ 3.7904400e-02 -1.1697840e-02]\n",
      " [ 5.4500150e-02 -5.1204210e-02]\n",
      " [ 2.2553570e-02 -1.9301360e-02]\n",
      " [ 4.1337500e-03 -2.2234560e-02]\n",
      " [ 1.2492780e-02 -5.1899260e-02]\n",
      " [-2.5649300e-03 -3.1220800e-02]\n",
      " [-5.8863300e-03 -4.5990970e-02]\n",
      " [-1.5057000e-02 -7.1843890e-02]\n",
      " [-1.2656890e-02 -4.1087600e-02]\n",
      " [-9.0281100e-03 -3.0906070e-02]\n",
      " [-3.0563740e-02 -1.2260207e-01]\n",
      " [-1.9475840e-02 -7.4954430e-02]\n",
      " [-1.3911420e-02 -9.7921690e-02]\n",
      " [-4.0666700e-03 -2.1733310e-02]\n",
      " [-1.6322800e-03 -1.7203910e-02]\n",
      " [-1.7426220e-02 -6.0177290e-02]\n",
      " [-5.1659400e-03 -2.1295870e-02]\n",
      " [-9.8146400e-03 -2.3175560e-02]\n",
      " [-2.3506130e-02 -5.6816180e-02]\n",
      " [-6.4833670e-02 -1.3140259e-01]\n",
      " [-3.0630200e-02 -5.8854330e-02]\n",
      " [-1.4022560e-02 -2.4230640e-02]\n",
      " [-6.6374000e-03 -1.3735600e-02]\n",
      " [-4.6104510e-02 -9.7924400e-02]\n",
      " [-1.4901680e-02 -5.1418970e-02]\n",
      " [-1.2407260e-02 -3.3051220e-02]\n",
      " [-9.2636900e-03 -3.5900600e-02]\n",
      " [-1.6370230e-02 -7.4465750e-02]\n",
      " [-1.0785390e-02 -6.3674600e-02]\n",
      " [-2.0830440e-02 -9.7614450e-02]\n",
      " [-2.0632280e-02 -1.0482940e-01]\n",
      " [-4.3711800e-03 -2.6744010e-02]\n",
      " [-1.2478420e-02 -7.6128160e-02]\n",
      " [-1.2822220e-02 -8.5667230e-02]\n",
      " [-2.0894080e-02 -1.6324508e-01]\n",
      " [-1.3650070e-02 -1.2200560e-01]\n",
      " [-2.4893500e-03 -2.1138250e-02]\n",
      " [-2.7844400e-03 -4.3092190e-02]\n",
      " [-4.0244100e-03 -8.6747580e-02]\n",
      " [-9.1666000e-04 -2.1870620e-02]\n",
      " [-1.6501000e-03 -4.2174380e-02]\n",
      " [-8.0829400e-03 -1.7044306e-01]\n",
      " [-2.4863400e-03 -7.8043430e-02]\n",
      " [-5.2228000e-04 -3.8398850e-02]\n",
      " [-3.1088600e-03 -2.2096810e-01]\n",
      " [-1.3187000e-04 -7.9327000e-03]\n",
      " [-2.2255000e-04 -8.6732600e-03]\n",
      " [-3.6317000e-04 -3.6284240e-02]\n",
      " [-1.5371200e-03 -1.1202103e-01]\n",
      " [-6.6438000e-04 -2.1554590e-02]\n",
      " [-4.4157000e-04 -6.2434110e-02]\n",
      " [-7.3917000e-04 -6.9854140e-02]\n",
      " [-8.7922000e-04 -1.0338770e-02]\n",
      " [-4.2201200e-03 -1.1704104e-01]\n",
      " [-1.7448200e-03 -2.5499110e-02]\n",
      " [-4.3221000e-04 -1.4571590e-02]\n",
      " [-4.0599600e-03 -9.7286410e-02]\n",
      " [ 1.5473100e-03 -6.7282630e-02]\n",
      " [-3.2998000e-04 -1.1952086e-01]\n",
      " [-3.1125400e-03 -1.3242519e-01]\n",
      " [-1.3830900e-03 -3.5628870e-02]\n",
      " [-1.9061000e-03 -1.2548525e-01]\n",
      " [-5.6584000e-04 -5.8055540e-02]\n",
      " [-2.0747000e-03 -6.8096040e-02]\n",
      " [-8.2395000e-04 -1.6711490e-02]\n",
      " [-3.6108000e-04 -5.9307000e-03]\n",
      " [-3.1314000e-04 -7.1884400e-03]\n",
      " [-1.5061800e-03 -1.2768800e-01]\n",
      " [-1.0065300e-03 -1.2024052e-01]\n",
      " [-2.5669500e-03 -4.0077210e-02]\n",
      " [-6.8156000e-04 -1.7063020e-02]\n",
      " [-1.7850400e-03 -6.2339280e-02]\n",
      " [-3.1696300e-03 -7.2574320e-02]\n",
      " [-4.7881000e-04 -4.9741930e-02]\n",
      " [ 1.8617800e-03 -1.4534153e-01]\n",
      " [-1.2733900e-03 -7.8307010e-02]\n",
      " [-1.4386400e-03 -2.4768220e-02]\n",
      " [-2.7483700e-03 -2.9403040e-02]\n",
      " [-6.5826300e-03 -1.1888777e-01]\n",
      " [-1.7452800e-03 -2.9493370e-02]\n",
      " [-5.6840000e-05 -1.2673400e-03]\n",
      " [-1.8048000e-04 -4.0238900e-03]\n",
      " [-1.9748200e-03 -2.5706050e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(new_stl_points[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
