{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import csv\n",
    "A = {'learning_rate':0.00009123,'keep_prob':0.25, 'activation':tf.nn.relu, 'channels':[16,16,32]}\n",
    "# A = {'a':'b','c':'d'}\n",
    "\n",
    "with open(\"output\"+\".csv\", \"w\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for key, val in A.items():\n",
    "        writer.writerow([key, val])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can add the parameter data_home to wherever to where you want to download your data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "((train_data, train_labels),(eval_data, eval_labels)) = tf.keras.datasets.mnist.load_data()\n",
    "train_data = train_data / np.float32(255)\n",
    "train_data = train_data.reshape(train_data.shape[0], -1)\n",
    "print(train_data.shape)\n",
    "train_labels = train_labels.astype(np.int32)  # not required\n",
    "print(train_labels.shape)\n",
    "# test_size: what proportion of original data is used for test set\n",
    "train_img, test_img, train_lbl, test_lbl = train_test_split(\n",
    "    train_data, train_labels, test_size=1/7.0, random_state=0)\n",
    "print(train_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvData = [(-0.53103447, [1e-05, 0.0, '1', 1]), (-0.53103447, [4.315202424267864e-05, 0.0, '1', 1]), (-0.53103447, [6.388961292348475e-05, 0.04720173848853115, '1', 4]), (-0.53103447, [0.0001540877645803547, 0.35419707255064065, '1', 1])]\n",
    "newData = []\n",
    "for i in csvData:\n",
    "    data = {'value': i[0],\n",
    "           'learning_rate': i[1][0],\n",
    "           'dropout_rate': i[1][1],\n",
    "           'activation': i[1][1],\n",
    "           'channels': i[1][1],}\n",
    "    newData.append(data)\n",
    "print(type(csvData))\n",
    "print((csvData))\n",
    "#print(type(csvData[0]))\n",
    "# print((csvData[0]))\n",
    "\n",
    "\n",
    "with open('person.csv', 'w') as csvFile:\n",
    "    writer = csv.DictWriter(csvFile, fieldnames = ['value','learning_rate','dropout_rate', 'activation', 'channels'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(newData)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Func vals: %s \\n\" % search_result.func_vals)\n",
    "print(\"Hyper para: %s \\n\" % search_result.x_iters)\n",
    "print(\"Combined: %s\" % list(zip(search_result.func_vals, search_result.x_iters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searched_parameter_2 = (list(zip(search_result.func_vals, search_result.x_iters)))\n",
    "i = searched_parameter_2[0]\n",
    "data_2 = {field_name[0]: i[0] * -1,\n",
    "                field_name[1]: i[1][0],\n",
    "                field_name[2]: i[1][1],\n",
    "                field_name[3]: i[1][1],\n",
    "                field_name[4]: i[1][1]}\n",
    "print(data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stl_points = points\n",
    "new_stl_points = []\n",
    "for stl_point_sample in stl_points:  # stl_point_sample is one example of stl_points\n",
    "    new_points_sample = []\n",
    "    for stl_point_image in stl_point_sample:  # stl_point_image is one degree of cross-section\n",
    "        difference = stl_point_image[1:, :] - stl_point_image[0:-1, :]  # Find difference between each position\n",
    "        new_points_sample.append(difference)\n",
    "    new_stl_points.append(new_points_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(new_stl_points))\n",
    "print(len(new_stl_points))\n",
    "print(type(new_stl_points[0]))\n",
    "print(len(new_stl_points[0]))\n",
    "print(type(new_stl_points[0][0]))\n",
    "print(np.shape(new_stl_points[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lbl_name)\n",
    "print(err_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# File name will be [tfrecord_name]_train_Taper_sum_median\n",
    "tfrecord_file_name = \"preparation_181_data\"\n",
    "# tfrecord_name = \"original_preparation_data\"\n",
    "# csv_name = \"../global_data/Ground Truth Score_50.csv\"\n",
    "# Directory of image\n",
    "dataset_folder_dir = \"./data/cross_section\"\n",
    " \n",
    "file_name = './data/preparation_181_data_train_address.txt'\n",
    "name = []\n",
    "if os.path.isfile(file_name):  # Check if file exist\n",
    "    with open(file_name) as f:\n",
    "        name = f.read().splitlines()\n",
    "    '''\n",
    "    with open(file_name, 'r') as filehandle:\n",
    "        for line in filehandle:\n",
    "            name.append(line)\n",
    "            \n",
    "            # remove linebreak which is the last character of the string\n",
    "            # current_name = line[:-1]\n",
    "            \n",
    "            # check if it exist in grouped exist, if found, put in train_address\n",
    "            for i, name in enumerate(example_grouped_address):\n",
    "                if current_name in name:\n",
    "                    train_address.append(grouped_address[i])\n",
    "                    grouped_address.remove(grouped_address[i])\n",
    "                    example_grouped_address.remove(example_grouped_address[i])\n",
    "                    break\n",
    "            '''\n",
    "    # print(\"Use %s examples from previous tfrecords as training\" % len(train_address))\n",
    "else:\n",
    "    print(\"Not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(name))\n",
    "for n in name[1:]:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (n[1] == '/home/pasin/Documents/Google_Drive/Aa_TIT_LAB_Comp/Library/Tooth/Tooth/Model/my2DCNN/data/cross_section/PreparationScan_1_85148')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readjust_median_label(label, avg_data):\n",
    "    possible_value = [1,3,5]\n",
    "    for i, label_value in enumerate(label):\n",
    "        if not (label_value in possible_value):\n",
    "            # Check if value is over/under boundary, if so, choose the min/max value\n",
    "            if label_value < possible_value[0]:\n",
    "                label[i] = possible_value[0]\n",
    "            elif label_value > possible_value[-1]:\n",
    "                label[i] = possible_value[-1]\n",
    "            else:\n",
    "                if label_value > avg_data[i]:  # If median is more than average, round up\n",
    "                    label[i] = min(filter(lambda x: x > label_value, possible_value))\n",
    "                else:  # If median is less or equal to average, around down\n",
    "                    label[i] = max(filter(lambda x: x < label_value, possible_value))\n",
    "    return label\n",
    "\n",
    "A = [1,3,5,2,2,4,4, 10, -2, 2]\n",
    "B = [1,4,2,3,1,4,7, 15]\n",
    "\n",
    "C = readjust_median_label(A,B)\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.load('/home/pasin/Documents/Google_Drive/Aa_TIT_LAB_Comp/Library/Tooth/Tooth/Model/my2DCNN/data/coordinates/PreparationScan_1_305139_0.npy')\n",
    "print(type(data))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def read_file(csv_dir, header = False):\n",
    "    header_name = []\n",
    "    data = []\n",
    "    with open(csv_dir) as csvfile:\n",
    "        readCSV = csv.reader(csvfile, delimiter=',')\n",
    "        for row in readCSV:\n",
    "            print(row)\n",
    "            if header:\n",
    "                header_name.append(row)\n",
    "                header = False\n",
    "            else:\n",
    "                data.append(row)\n",
    "    if not header_name:\n",
    "        return data\n",
    "    else:\n",
    "        return data, header_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, header = read_file(file_name[-2], True)\n",
    "print(header)\n",
    "print(\"\\n\")\n",
    "print(data[-2])\n",
    "l_data = data[-2][1:]\n",
    "print([float(l_data[0]), float(l_data[1]),l_data[2],int(l_data[3]),int(l_data[3])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "labels = tf.constant([0,1,2,0])\n",
    "labels_one_hot = tf.one_hot(labels,3)\n",
    "\n",
    "weight = tf.constant([[1,2,3]], dtype=tf.float32)\n",
    "new_w = tf.matmul(labels_one_hot,weight,transpose_b=True)\n",
    "\n",
    "logits = tf.constant([[0,1,0],[0,1,0],[0,1,0],[1,0,0]], dtype  =tf.float32)\n",
    "\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(labels, logits, weights = new_w)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "print(sess.run(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "def read_npy_file(item):\n",
    "    print(\"Item: %s\" % item)\n",
    "    label = item[1]\n",
    "    image = []\n",
    "    for i in range(4):\n",
    "        img = np.load(item[0][i])\n",
    "        image.append(img.astype(np.float32))\n",
    "    return (image, label)\n",
    "\n",
    "A = \"/home/pasin/Documents/Link to Tooth/Tooth/Model/my2DCNN/data/coordinate_300_point/\"\n",
    "B = tf.py_function(read_npy_file, [A], [tf.float32, ])\n",
    "\n",
    "sess = tf.Session()\n",
    "print(sess.run(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_save_file import save_file\n",
    "\n",
    "field_name = ['accuracy', 'learning_rate', 'dropout_rate', 'activation', 'cnn_channels', 'fc_channels']\n",
    "save_file('/home/pasin/Documents/Pasin/model/BL_classification/hyperparameters_result_20190424_000.csv', [], field_name = field_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.space import Real, Categorical, Integer\n",
    "dim_learning_rate = Real(low=1e-5, high=1e-2, prior='log-uniform', name='learning_rate')\n",
    "dim_dropout_rate = Real(low=0, high=0.875, name='dropout_rate')\n",
    "dim_activation = Categorical(categories=['0', '1'],\n",
    "                             name='activation')\n",
    "dim_channel = Integer(low=1, high=4, name='channels')\n",
    "dim_channel_fc = Integer(low=1, high=2, name='fully_connect_channels')  # Fully conencted\n",
    "dim_loss_weight = Real(low=0.8, high=2, name='loss_weight')\n",
    "dimensions = [dim_learning_rate,\n",
    "              dim_dropout_rate,\n",
    "              dim_activation,\n",
    "              dim_channel,\n",
    "              dim_channel_fc,\n",
    "              dim_loss_weight]\n",
    "\n",
    "dim_name = [i.name for i in dimensions]\n",
    "print(dim_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "numdegree = 4\n",
    "\n",
    "def create_file_reader_ops(filename_queue):\n",
    "    reader = tf.TextLineReader(skip_header_lines=1)\n",
    "    _, csv_row = reader.read(filename_queue)\n",
    "    record_defaults = [[0], [0]]\n",
    "    x,y = tf.decode_csv(csv_row, record_defaults=record_defaults)\n",
    "    features = tf.pack([x,y])\n",
    "    return features\n",
    "\n",
    "\n",
    "# Import tfrecord to dataset\n",
    "def deserialize(example):\n",
    "    feature = {'label': tf.FixedLenFeature([], tf.int64)}\n",
    "    for i in range(numdegree):\n",
    "        feature['img' + str(i)] = tf.FixedLenFeature([], tf.string)\n",
    "    return tf.parse_single_example(example, feature)\n",
    "\n",
    "\n",
    "def decode(data_dict):\n",
    "    if numdegree != 4:\n",
    "        raise Exception('Number of degree specified is not compatible, edit code')\n",
    "    # Create initial image, then stacking it\n",
    "    image_decoded = []\n",
    "\n",
    "    # Stacking the rest\n",
    "    for i in range(0, numdegree):\n",
    "        img = data_dict['img' + str(i)]\n",
    "        file_name = tf.read_file(img)\n",
    "        filename_queue = tf.train.string_input_producer([file_name])\n",
    "        example = create_file_reader_ops(filename_queue)\n",
    "        image_decoded.append(example)\n",
    "    \n",
    "    # image_stacked = tf.stack([image_decoded[0], image_decoded[1], image_decoded[2], image_decoded[3]], axis=2)\n",
    "    # image_stacked = tf.cast(image_stacked, tf.float32)\n",
    "    label = tf.cast(data_dict['label'], tf.float32)\n",
    "    # output = (image_stacked, label)\n",
    "    # return {'images': image_stacked, 'label': label}  # Output is [Channel, Height, Width]\n",
    "    return image_decoded[0], label# image_stacked, label\n",
    "\n",
    "data_path = \"/home/pasin/Documents/Link to Tooth/Tooth/Model/my2DCNN/data/tfrecord/preparation_coor_debug_BL_median_coor_train.tfrecords\"\n",
    "dataset = tf.data.TFRecordDataset(data_path)\n",
    "dataset = dataset.map(deserialize, num_parallel_calls=7)\n",
    "dataset = dataset.map(decode, num_parallel_calls=7)\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_image_data = iterator.get_next()\n",
    "images = []\n",
    "label = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    try:\n",
    "        # Keep extracting data till TFRecord is exhausted\n",
    "        while True:\n",
    "            data = sess.run(next_image_data)\n",
    "            images.append(data[0])\n",
    "            label.append(data[1])\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file_reader_ops(filename_queue):\n",
    "    reader = tf.TextLineReader(skip_header_lines=1)\n",
    "    _, csv_row = reader.read(filename_queue)\n",
    "    record_defaults = [[0], [0]]\n",
    "    x,y = tf.decode_csv(csv_row, record_defaults=record_defaults)\n",
    "    features = tf.pack([x,y])\n",
    "    return features\n",
    "filename_queue = tf.train.string_input_producer([\"/home/pasin/Documents/Link to Tooth/Tooth/Model/my2DCNN/data/tfrecord/preparation_coor_debug_BL_median_coor_train.tfrecords\"], num_epochs=1, shuffle=False)\n",
    "example = create_file_reader_ops(filename_queue)\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "A = np.array([[1,2],[3,4],[5,6],[7,8],[9,10]])\n",
    "B = tf.sparse_tensor_to_dense(A,default_value=0)\n",
    "C= tf.reshape(B,[10])\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "print(sess.run(B))\n",
    "print(sess.run(C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Read TFRecord file\n",
    "reader = tf.TFRecordReader()\n",
    "filename_queue = tf.train.string_input_producer(['/home/pasin/Documents/Link to Tooth/Tooth/Model/my2DCNN/data/tfrecord/preparation_coor_smallraw_Taper_Sum_median_coor_train.tfrecords'])\n",
    "\n",
    "_, serialized_example = reader.read(filename_queue)\n",
    "numdegree = 4\n",
    "# Define features\n",
    "def deserialize(example):\n",
    "    feature = {'label': tf.FixedLenFeature([], tf.int64)}\n",
    "    for i in range(numdegree):\n",
    "        for j in range(2):\n",
    "            feature['img' + str(i) + str(j)] = tf.FixedLenFeature([], tf.float32)\n",
    "    return tf.parse_single_example(example, feature)\n",
    "\n",
    "# Extract features from serialized data\n",
    "read_data = deserialize(filename_queue)\n",
    "\n",
    "# Many tf.train functions use tf.train.QueueRunner,\n",
    "# so we need to start it before we read\n",
    "tf.train.start_queue_runners(sess)\n",
    "\n",
    "# Print features\n",
    "with tf.Session() as sess:\n",
    "    for name, tensor in sess.run(read_data).items():\n",
    "        print('{}: {}'.format(name, tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_data import get_data_from_path\n",
    "\n",
    "\n",
    "\n",
    "p = \"/home/pasin/Documents/Link to Tooth/Tooth/Model/my2DCNN/data/tfrecord/preparation_361_MD_median_train.tfrecords\"\n",
    "img = get_data_from_path(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_save_file import get_label\n",
    "import collections\n",
    "label, label_name = get_label(\"BL\",\"median\",double_data=True,one_hotted=False,normalized=False)\n",
    "\n",
    "counter=collections.Counter(label)\n",
    "for k,v in counter.items():\n",
    "    print(k)\n",
    "    print(v)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "weight = tf.constant([[1,2, 1]],dtype=tf.float32)\n",
    "loss_weight = tf.matmul(one_hot_label,weight,transpose_b=True,a_is_sparse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "csv_dir = \"/home/pasin/Documents/Pasin/model/BL_coordinate_dense/hyperparameters_result_20190521_17_42_43.csv\"\n",
    "print(os.path.dirname(csv_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coor_get_data import get_data_from_path\n",
    "\n",
    "image, labels = get_data_from_path('./data/tfrecord/preparation_coor_300/preparation_coor_300_BL_median_train.tfrecords', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "labels = tf.cast(labels, tf.int64)\n",
    "labels = (labels-1)/2\n",
    "print(labels)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(labels))\n",
    "one_hot_label = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=3)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(one_hot_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "\n",
    "def softmax_focal_loss(labels_l, logits_l, gamma=2., alpha=4.):\n",
    "    \"\"\"Focal loss for multi-classification\n",
    "    https://www.dlology.com/blog/multi-class-classification-with-focal-loss-for-imbalanced-datasets/\n",
    "    FL(p_t)=-alpha(1-p_t)^{gamma}ln(p_t)\n",
    "    gradient is d(Fl)/d(p_t) not d(Fl)/d(x) as described in paper\n",
    "    d(Fl)/d(p_t) * [p_t(1-p_t)] = d(Fl)/d(x)\n",
    "    Focal Loss for Dense Object Detection\n",
    "    https://arxiv.org/abs/1708.02002\n",
    "\n",
    "    Arguments:\n",
    "        labels_l {tensor} -- ground truth labels_l, shape of [batch_size, num_class] <- One hot vector\n",
    "        logits_l {tensor} -- model's output, shape of [batch_size, num_class] <- Before softmax\n",
    "\n",
    "    Keyword Arguments:\n",
    "        gamma {float} -- (default: {2.0})\n",
    "        alpha {float} -- (default: {4.0})\n",
    "\n",
    "    Returns:\n",
    "        [tensor] -- loss.\n",
    "    \"\"\"\n",
    "\n",
    "    gamma = float(gamma)\n",
    "\n",
    "    epsilon = 1e-32\n",
    "    # labels_l = tf.cast(labels_l, tf.float32)\n",
    "    labels_l = tf.one_hot(indices=tf.cast(labels_l, tf.int32), depth=3)\n",
    "    logits_l = tf.cast(logits_l, tf.float32)\n",
    "\n",
    "    # logits_l = tf.nn.softmax(logits_l)\n",
    "    # print(\"Softmax: %s\" % logits_l)\n",
    "    logits_l = tf.add(logits_l, epsilon)  # Add epsilon so log is valid\n",
    "    ce = tf.multiply(labels_l, -tf.log(logits_l))  # Cross entropy, shape of [batch_size, num_class]\n",
    "    fl_weight = tf.multiply(labels_l, tf.pow(tf.subtract(1., logits_l), gamma))  # This is focal loss part\n",
    "    fl = tf.multiply(alpha, tf.multiply(fl_weight, ce))  # Add alpha weight here\n",
    "    reduced_fl = tf.reduce_max(fl, axis=1)\n",
    "    return tf.reduce_mean(reduced_fl)\n",
    "\n",
    "\n",
    "# labels = tf.constant([1,5,3])\n",
    "# logits = tf.constant([[50,0,0],[1,1,2],[90,0,90]],dtype=tf.float32)\n",
    "x = []\n",
    "y = []\n",
    "y2 = []\n",
    "y3 = []\n",
    "y4 = []\n",
    "\n",
    "for i in np.linspace(0, 1, 1001):\n",
    "    x.append(i)\n",
    "    labels = tf.constant([1])\n",
    "    logits = tf.constant([i, 0, 1 - i])\n",
    "    labels = (labels - 1) / 2\n",
    "    one_hot_label = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=3)\n",
    "    labels = tf.cast(labels, tf.int64)\n",
    "\n",
    "    weight = tf.constant([[1, 1, 1]],\n",
    "                         dtype=tf.float32)\n",
    "    loss_weight = tf.linalg.matmul(one_hot_label, weight, transpose_b=True, a_is_sparse=True)\n",
    "\n",
    "    loss_fc = softmax_focal_loss(labels, logits, gamma=0, alpha=loss_weight)  # labels is int of class, logits is vector\n",
    "    y.append(loss_fc)\n",
    "\n",
    "    loss_fc = softmax_focal_loss(labels, logits, gamma=1, alpha=loss_weight)  # labels is int of class, logits is vector\n",
    "    y2.append(loss_fc)\n",
    "\n",
    "    loss_fc = softmax_focal_loss(labels, logits, gamma=2, alpha=loss_weight)  # labels is int of class, logits is vector\n",
    "    y3.append(loss_fc)\n",
    "\n",
    "    loss_fc = softmax_focal_loss(labels, logits, gamma=5, alpha=loss_weight)  # labels is int of class, logits is vector\n",
    "    y4.append(loss_fc)\n",
    "    # loss_ce = tf.losses.sparse_softmax_cross_entropy(labels, logits,\n",
    "    #                                               weights=loss_weight)  # labels is int of class, logits is vector\n",
    "    # print(\"Focal loss:\")\n",
    "    # print(loss_fc)\n",
    "    # print(\"Cross entropy\")\n",
    "    # print(loss_ce)\n",
    "    # print(\"\" )\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     print(sess.run(loss_fc))\n",
    "#     print(sess.run(loss_ce))\n",
    "print(\"debug\")\n",
    "fig = plt.figure(figsize=(18, 10), dpi=50, facecolor='w', edgecolor='k')\n",
    "plt.plot(x, y, color='skyblue', label='Cross Entropy')\n",
    "plt.plot(x, y2, color='olive', label='gamma = 1')\n",
    "plt.plot(x, y3, color='green', label='gamma = 2')\n",
    "plt.plot(x, y4, color='red', label='gamma = 5')\n",
    "#plt.legend(loc=2, prop={'size': 6})\n",
    "# naming the x axis\n",
    "plt.xlabel('Probability', fontsize=40)\n",
    "# plt.xlim(0, 1)\n",
    "# naming the y axis\n",
    "plt.ylabel('Loss', fontsize=40)\n",
    "# plt.ylim(0, 5)\n",
    "# giving a title to my graph\n",
    "plt.title('Focal Loss', fontsize=40)\n",
    "plt.legend()\n",
    "# function to show the plot\n",
    "# plt.show()\n",
    "\n",
    "plt.savefig('focal_loss.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from open_save_file import get_input_and_label\n",
    "from sklearn.model_selection import KFold\n",
    "numdeg = 4  # Number of images on each example\n",
    "\n",
    "configs = {'numdeg': numdeg,\n",
    "           'train_eval_ratio': 0.8,\n",
    "           'label_data': \"Taper_Sum\",\n",
    "           'label_type': \"median\"\n",
    "           }\n",
    "\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def _float_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "\n",
    "# Run images from pre_processing.py into tfrecords\n",
    "def serialize_image(example):\n",
    "    feature = {'label': _int64_feature(example['label'])}\n",
    "    for i in range(numdeg):\n",
    "        feature['img' + str(i)] = _bytes_feature(example['img' + str(i)])\n",
    "\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return tf_example.SerializeToString()\n",
    "\n",
    "\n",
    "def read_image(file_name, label):\n",
    "    file_values = {'label': label}\n",
    "    for i in range(numdeg):\n",
    "        file_values['img' + str(i)] = tf.read_file(file_name[i])\n",
    "    return file_values\n",
    "\n",
    "\n",
    "'''\n",
    "# Read and load image\n",
    "def load_image(addr):\n",
    "    img = cv2.imread(addr, cv2.IMREAD_GRAYSCALE)\n",
    "    # cv2.imshow('IMAGE WINDOW',img)\n",
    "    # cv2.waitKey()\n",
    "    # print(np.shape(img))\n",
    "    return img\n",
    "'''\n",
    "\n",
    "\n",
    "def image_to_tfrecord(tfrecord_name, dataset_folder, csv_dir=None):\n",
    "    # Create new directory if not created, get all info and zip to tfrecord\n",
    "    tfrecord_dir = os.path.join(\"./data/tfrecord\", tfrecord_name)\n",
    "    if not os.path.exists(tfrecord_dir):\n",
    "        os.makedirs(tfrecord_dir)\n",
    "\n",
    "    #Get file name from dataset_folder\n",
    "    grouped_train_address, grouped_eval_address = get_input_and_label(tfrecord_name, dataset_folder, csv_dir, configs)\n",
    "    # Start writing train dataset\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(grouped_train_address)\n",
    "    train_dataset = train_dataset.map(read_image)  # Read file address, and get info as string\n",
    "\n",
    "    it = train_dataset.make_one_shot_iterator()\n",
    "\n",
    "    elem = it.get_next()\n",
    "\n",
    "\n",
    "\n",
    "    tfrecord_train_name = os.path.join(tfrecord_dir, \"%s_%s_%s_train.tfrecords\" % (\n",
    "        tfrecord_name, configs['label_data'], configs['label_type']))\n",
    "    tfrecord_eval_name = os.path.join(tfrecord_dir, \"%s_%s_%s_eval.tfrecords\" % (\n",
    "        tfrecord_name, configs['label_data'], configs['label_type']))\n",
    "    # eval_score_name = os.path.join(\"./data/tfrecord\", \"%s_%s_%s_score.npy\" % (\n",
    "    #     tfrecord_name, configs['label_data'], configs['label_type']))\n",
    "    # np.save(eval_score_name, np.asarray(eval_score))\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        writer = tf.python_io.TFRecordWriter(tfrecord_train_name)\n",
    "        while True:\n",
    "            try:\n",
    "                elem_result = serialize_image(sess.run(elem))\n",
    "\n",
    "                writer.write(elem_result)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "        writer.close()\n",
    "\n",
    "    eval_dataset = tf.data.Dataset.from_tensor_slices(grouped_eval_address)\n",
    "    eval_dataset = eval_dataset.map(read_image)\n",
    "\n",
    "    it = eval_dataset.make_one_shot_iterator()\n",
    "\n",
    "    elem = it.get_next()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        writer = tf.python_io.TFRecordWriter(tfrecord_eval_name)\n",
    "        while True:\n",
    "            try:\n",
    "                elem_result = serialize_image(sess.run(elem))\n",
    "                # print(elem_result)\n",
    "                writer.write(elem_result)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "        writer.close()\n",
    "    print(\"TFrecords created: %s, %s\" % (tfrecord_train_name, tfrecord_eval_name))\n",
    "\n",
    "\n",
    "# Run images from pre_processing.py into tfrecords\n",
    "def serialize_coordinate(example):\n",
    "    feature = {'label': _int64_feature(example['label'])}\n",
    "    for i in range(numdeg):\n",
    "        feature['img' + str(i)] = _float_feature(example['img' + str(i)])\n",
    "\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return tf_example.SerializeToString()\n",
    "\n",
    "\n",
    "def read_coordinate(file_name, label):\n",
    "    file_values = {'label': label}\n",
    "    for i in range(numdeg):\n",
    "        file_values['img' + str(i)] = (file_name[i]*10000).tostring()\n",
    "    return file_values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "tfrecord_name   : Name of .tfrecord output file\n",
    "dataset_folder  : Folder of the input data  (Not include label)\n",
    "csv_dir         : Folder of label data (If not specified, will use the default directory)\n",
    "\"\"\"\n",
    "# Create new directory if not created, get all info and zip to tfrecord\n",
    "# tfrecord_dir = os.path.join(\"./data/tfrecord\", tfrecord_name)\n",
    "# if not os.path.exists(tfrecord_dir):\n",
    "#     os.makedirs(tfrecord_dir)\n",
    "\n",
    "# Get data from dataset_folder\n",
    "grouped_data = []\n",
    "for i in range(5):\n",
    "    grouped_data.append([[i*10+0.1,i*10+0.2,i*10+0.3],10*i])\n",
    "grouped_data = tuple(grouped_data)\n",
    "\n",
    "# print(grouped_data)\n",
    "kfold = KFold(5, shuffle = True, random_state = 2)\n",
    "\n",
    "for train, test in kfold.split(grouped_data):\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    for train_indice in train:\n",
    "        train_data.append(grouped_data[train_indice])\n",
    "    for test_indice in test:\n",
    "        test_data.append(grouped_data[test_indice])\n",
    "    print(train_data)\n",
    "    print(test_data)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pre_processing import get_cross_section\n",
    "\n",
    "stl_points_all, label_all, label_name_all, error_file_names_all, degree, augment_config = get_cross_section(\"BL\", \"median\",\n",
    "                                                                                                            folder_name='/home/pasin/Documents/Link to Tooth/Tooth/Model/global_data/stl_data',\n",
    "                                                                                                            csv_dir=\"/home/pasin/Documents/Link to Tooth/Tooth/Model/global_data/Ground Truth Score_new.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(label_all))\n",
    "print(len(label_all[0]))\n",
    "print((error_file_names_all[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from stl import mesh\n",
    "import open3d\n",
    "import pptk\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "def stl_to_point(v1, v2, v3, num_points, sampling_mode=\"weight\"):\n",
    "    \"\"\"\n",
    "    Function to convert stl file into point cloud\n",
    "    https://medium.com/@daviddelaiglesiacastro/3d-point-cloud-generation-from-3d-triangular-mesh-bbb602ecf238\n",
    "    :param v1, v2, v3 : (N,3) ndarrays, vi represent x,y,z coordinates of one vertex\n",
    "    :param num_points: Number of points we want to sample\n",
    "    :param sampling_mode: String, type of sampling from triangle, recommend \"weight\"\n",
    "    :return: points: numpy array of point cloud\n",
    "    \"\"\"\n",
    "    if not(np.shape(v1)[0] ==  np.shape(v2)[0] ==  np.shape(v3)[0]):\n",
    "        raise ValueError(\"Size of all three vertex is not the same\")\n",
    "    else:\n",
    "        print(\"Number of mesh: %s\" % np.shape(v1)[0])\n",
    "    areas = triangle_area_multi(v1, v2, v3)\n",
    "    prob = areas / areas.sum()\n",
    "    if sampling_mode == \"weight\":\n",
    "        indices = np.random.choice(range(len(areas)), size=num_points, p=prob)\n",
    "    else:\n",
    "        indices = np.random.choice(range(len(areas)), size=num_points)\n",
    "    points = select_point_from_triangle(v1[indices, :], v2[indices, :], v3[indices, :])\n",
    "    return points\n",
    "\n",
    "\n",
    "def triangle_area_multi(v1, v2, v3):\n",
    "    \"\"\"\n",
    "    Find area of triangle, used for finding weights\n",
    "    :param v1, v2, v3 : (N,3) ndarrays, vi represent x,y,z coordinates of one vertex\n",
    "    :return: size of triangle\n",
    "    \"\"\"\n",
    "    return 0.5 * np.linalg.norm(np.cross(v2 - v1, v3 - v1), axis=1)\n",
    "\n",
    "\n",
    "def select_point_from_triangle(v1, v2, v3):\n",
    "    \"\"\"\n",
    "    Select one point from each three vertex\n",
    "    :param v1, v2, v3 : (N,3) ndarrays, vi represent x,y,z coordinates of one vertex\n",
    "    :return: ndarrays\n",
    "    \"\"\"\n",
    "    n = np.shape(v1)[0]\n",
    "    u = np.random.rand(n, 1)\n",
    "    v = np.random.rand(n, 1)\n",
    "    is_a_problem = u + v > 1\n",
    "\n",
    "    u[is_a_problem] = 1 - u[is_a_problem]\n",
    "    v[is_a_problem] = 1 - v[is_a_problem]\n",
    "\n",
    "    w = 1 - (u + v)\n",
    "\n",
    "    points = (v1 * u) + (v2 * v) + (v3 * w)\n",
    "\n",
    "    return points\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    a = mesh.Mesh.from_file(\n",
    "        \"/home/pasin/Documents/Link to Tooth/Tooth/Model/global_data/stl_data/84101-2/PreparationScan.stl\")\n",
    "\n",
    "    # v1 = np.array([[1, 0, 0], [2, 3, 1], [-2, 5, 0]])\n",
    "    # v2 = np.array([[0, 0, 0], [2, 4, 2], [0, 5, 4]])\n",
    "    # v3 = np.array([[0, 1, 0], [2, 4, 1], [2, 5, 0]])\n",
    "\n",
    "    point = stl_to_point(a.v0, a.v1, a.v2, 2000)\n",
    "    # point = stl_to_point(a.v0, a.v1, a.v2, 2000, sampling_mode=None)\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes(projection='3d')\n",
    "    \n",
    "    ax.set_xlim([-4,4])\n",
    "    ax.set_ylim([-4,4]) \n",
    "    ax.set_zlim([-4,4]) \n",
    "    ax.scatter3D(point[:,0], point[:,2], point[:,1], cmap='Greens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stlSlicer.py version: 1.8.1\n",
      "get_file_name: Import from /home/pasin/Documents/Google_Drive/Aa_TIT_LAB_Comp/Library/Tooth/Tooth/Model/global_data/stl_data, searching for file name with PreparationScan.stl\n",
      "get_file_name: Uploaded 365 file names\n",
      "Progress: 10, current image: 84134\n",
      "Progress: 20, current image: 84144\n",
      "Progress: 30, current image: 85110\n",
      "Progress: 40, current image: 85120\n",
      "Progress: 50, current image: 85130\n",
      "Progress: 60, current image: 85140\n",
      "Progress: 70, current image: 85151\n",
      "getSlicer: /home/pasin/Documents/Google_Drive/Aa_TIT_LAB_Comp/Library/Tooth/Tooth/Model/global_data/stl_data/94106-2/PreparationScan.stl has problem getting cross-section. Possible hole appeared in model\n",
      "Progress: 80, current image: 94110\n",
      "Progress: 90, current image: 94120\n",
      "Progress: 100, current image: 94130\n",
      "Progress: 110, current image: 94141\n",
      "Progress: 120, current image: 95101\n",
      "Progress: 130, current image: 95112\n",
      "Progress: 140, current image: 95125\n",
      "Progress: 150, current image: 95138\n",
      "Progress: 160, current image: 95148\n",
      "Progress: 170, current image: 95209\n",
      "Progress: 180, current image: 95221\n",
      "Progress: 190, current image: 95234\n",
      "Progress: 200, current image: 95245\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from stl import mesh\n",
    "import sys\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from stlSlicer import getSlicer\n",
    "from open_save_file import get_file_name\n",
    "\n",
    "#TODO; Check if all file is created\n",
    "\n",
    "augment_config = [0, 5, 10, 15, 20, 25, 30, 35, 40]\n",
    "degree = [0, 45, 90, 135]\n",
    "\n",
    "\n",
    "def find_slope(point1, point2):  # function to find top line\n",
    "    if point1[0] == point2[0]:\n",
    "        return 0\n",
    "    else:\n",
    "        return (point1[1] - point2[1]) / (point1[0] - point2[0])\n",
    "    \n",
    "        \n",
    "def get_segment(points, mode=None, margin = 0, file_name = None):\n",
    "    \"\"\"\n",
    "    Get a segments of cross section\n",
    "    :param points: ndarray of N*2\n",
    "    :param mode: String, [left, right, top, debug] -> select which part to return\n",
    "    :param margin: Float, amount of margin from minimum line \n",
    "    :return: ndarray of segmented image or saved image (Not sure)\n",
    "    \"\"\"\n",
    "    # Turn into numpy\n",
    "    points = np.asarray(points)\n",
    "    \n",
    "    # Find max and min x to find sharp point of tooth\n",
    "    x_min_index = np.argmin(points,axis=0)[0]\n",
    "    x_max_index = np.argmax(points,axis=0)[0]  \n",
    "    \n",
    "    left_point = points[x_min_index,:]\n",
    "    right_point = points[x_max_index,:]\n",
    "\n",
    "    # Initial slope, (+20 points to avoid corner of the curve which could provide a unusually large slope)\n",
    "    top_left_index = x_min_index + 20\n",
    "    top_right_index = x_max_index - 20\n",
    "    left_slope = find_slope(left_point, points[top_left_index])\n",
    "    right_slope= find_slope(right_point, points[top_right_index])\n",
    "    slope_track = []\n",
    "    # Find largest slope point, choose as top_left and top_right\n",
    "    for i in range(top_left_index, top_right_index):\n",
    "        # For left side, find the steepest slope\n",
    "        new_left_slope = find_slope(points[x_min_index,:], points[i,:])\n",
    "        if  new_left_slope > left_slope and new_left_slope > 0:\n",
    "            left_slope = new_left_slope\n",
    "            top_left_index = i\n",
    "        # For right side, find the steepest negative slope\n",
    "        new_right_slope = find_slope(points[x_max_index,:], points[i,:])\n",
    "        slope_track.append([new_right_slope,points[i,:]])\n",
    "        if  new_right_slope < right_slope and new_right_slope < 0:\n",
    "            right_slope = new_right_slope\n",
    "            top_right_index = i\n",
    "            \n",
    "    # Current coordinates: x_min_index, top_left_index, x_max_index, top_right_index\n",
    "\n",
    "\n",
    "#     # saves the points from condition above\n",
    "#     FigureCoords = np.asarray(B)\n",
    "#     np.save(file=save_path + '1dfigs/' + directory[1] + '_' + str(cross_sections[ind]), arr=FigureCoords)\n",
    "\n",
    "    if mode == \"left\":\n",
    "        # If there is margin, choose points slightly below x_min_index\n",
    "        if margin > 0:\n",
    "            segmented_points = points[0:top_left_index+1,:]\n",
    "            segmented_points = segmented_points[segmented_points[:,1]>points[x_min_index,1] - margin]\n",
    "        else:\n",
    "            segmented_points = points[x_min_index:top_left_index+1,:]\n",
    "        x = segmented_points[:,0]\n",
    "        y = segmented_points[:,1]\n",
    "    elif mode == \"right\":\n",
    "        if margin > 0:\n",
    "            segmented_points = points[top_right_index:,:]\n",
    "            segmented_points = segmented_points[segmented_points[:,1]>points[x_max_index, 1] - margin]\n",
    "        else:\n",
    "            segmented_points = points[top_right_index:x_max_index+1,:]\n",
    "        x = segmented_points[:,0]\n",
    "        y = segmented_points[:,1]\n",
    "    elif mode == \"top\":\n",
    "        segmented_points = points[top_left_index:top_right_index+1,:]\n",
    "        x = segmented_points[:,0]\n",
    "        y = segmented_points[:,1]\n",
    "    else:\n",
    "        # Display entire tooth\n",
    "        mode = None\n",
    "        segmented_points = points\n",
    "        x = points[:,0]\n",
    "        y = points[:,1]\n",
    "        \n",
    "    # print(np.shape(x))\n",
    "\n",
    "    # Plotting\n",
    "    dpi = 100\n",
    "    img_size = 800\n",
    "    fig = plt.figure(figsize=(img_size / dpi, img_size / dpi), dpi=dpi)\n",
    "    ax = fig.gca()\n",
    "    min_x, max_x, min_y, max_y = -5, 5, -6, 6\n",
    "    if mode is None:\n",
    "        ax.axis([min_x, max_x, min_y, max_y])\n",
    "        ax.set_autoscale_on(False)  # allows us to define scale\n",
    "    ax.plot(x, y,linewidth=1.0)\n",
    "\n",
    "    if file_name is not None:\n",
    "        #plot lines for viewing\n",
    "        x1 = range(-5,5)\n",
    "        yleft = np.full((10,), points[top_left_index,:][1])\n",
    "        yright = np.full((10,), points[top_right_index,:][1])\n",
    "        ax.plot(x1, yleft, '-c')\n",
    "        ax.plot(x1, yright, '-r')\n",
    "        if (mode is None) or (mode == \"left\"):\n",
    "            ybottom_left = np.full((10,), points[x_min_index,:][1]-margin)\n",
    "            ax.plot(x1, ybottom_left, '-py')\n",
    "            ybottom_left = np.full((10,), points[x_min_index,:][1])\n",
    "            ax.plot(x1, ybottom_left, '-y')\n",
    "        if (mode is None) or (mode == \"right\"):\n",
    "            ybottom_right = np.full((10,), points[x_max_index,:][1]-margin)\n",
    "            ax.plot(x1, ybottom_right, '-pb')\n",
    "            ybottom_right = np.full((10,), points[x_max_index,:][1])\n",
    "            ax.plot(x1, ybottom_right, '-b')\n",
    "        fig.savefig(file_name, bbox_inches='tight')\n",
    "        plt.close('all')\n",
    "    return segmented_points\n",
    "\n",
    "\n",
    "def get_segment_multiple(name, margin = 0, base_dir = \"/home/pasin/Documents/Google_Drive/Aa_TIT_LAB_Comp/Library/Tooth/Tooth/Model/my2DCNN/data/segment_2\"):\n",
    "    \"\"\"\n",
    "    Get a segments of cross section from multiple files\n",
    "    :param points: ndarray of N*2\n",
    "    :param mode: String, [left, right, top, debug] -> select which part to return\n",
    "    :param margin: Float, amount of margin from minimum line \n",
    "    :return: ndarray of segmented image or saved image (Not sure)\n",
    "    \"\"\"\n",
    "    name_dir, image_name = get_file_name(folder_name=name, file_name=\"PreparationScan.stl\")\n",
    "    name_dir = name_dir[160:]\n",
    "    image_name = image_name[160:]\n",
    "    cnt = 0\n",
    "    for n_name,im_name in zip(name_dir,image_name):\n",
    "        cnt += 1\n",
    "        points_all = getSlicer(n_name, 0, degree,augment=augment_config, axis=1)\n",
    "        stl_points = []\n",
    "        error_file_names = []  # Names of file that cannot get cross-section image\n",
    "        \n",
    "        for index, point in enumerate(points_all): # Enumerate over all augmentation points\n",
    "            if point is None:\n",
    "                error_file_names.append(\"PreparationScan_%s_%d\" % (im_name,augment_config[index]))\n",
    "            else:\n",
    "                for d in range(len(degree)):\n",
    "                    \n",
    "                    file_name = \"PreparationScan_%s_%s_%d.png\" % (im_name,augment_config[index], degree[d])\n",
    "                    file_name_point = \"PreparationScan_%s_%s_%d.npy\" % (im_name, augment_config[index],degree[d])\n",
    "                    # Turn into numpy\n",
    "                    points = np.asarray(point[d])\n",
    "\n",
    "                    # Find max and min x to find sharp point of tooth\n",
    "                    x_min_index = np.argmin(points,axis=0)[0]\n",
    "                    x_max_index = np.argmax(points,axis=0)[0]  \n",
    "\n",
    "                    left_point = points[x_min_index,:]\n",
    "                    right_point = points[x_max_index,:]\n",
    "\n",
    "                    # Initial slope, (+20 points to avoid corner of the curve which could provide a unusually large slope)\n",
    "                    top_left_index = x_min_index + 20\n",
    "                    top_right_index = x_max_index - 20\n",
    "                    left_slope = find_slope(left_point, points[top_left_index])\n",
    "                    right_slope= find_slope(right_point, points[top_right_index])\n",
    "                    slope_track = []\n",
    "                    # Find largest slope point, choose as top_left and top_right\n",
    "                    for i in range(top_left_index, top_right_index):\n",
    "                        # For left side, find the steepest slope\n",
    "                        new_left_slope = find_slope(points[x_min_index,:], points[i,:])\n",
    "                        if  new_left_slope > left_slope and new_left_slope > 0:\n",
    "                            left_slope = new_left_slope\n",
    "                            top_left_index = i\n",
    "                        # For right side, find the steepest negative slope\n",
    "                        new_right_slope = find_slope(points[x_max_index,:], points[i,:])\n",
    "                        slope_track.append([new_right_slope,points[i,:]])\n",
    "                        if  new_right_slope < right_slope and new_right_slope < 0:\n",
    "                            right_slope = new_right_slope\n",
    "                            top_right_index = i\n",
    "\n",
    "                    # Current coordinates: x_min_index, top_left_index, x_max_index, top_right_index\n",
    "\n",
    "\n",
    "                #     # saves the points from condition above\n",
    "                #     FigureCoords = np.asarray(B)\n",
    "                #     np.save(file=save_path + '1dfigs/' + directory[1] + '_' + str(cross_sections[ind]), arr=FigureCoords)\n",
    "                    \n",
    "                    # Plotting\n",
    "                    dpi = 100\n",
    "                    img_size = 800\n",
    "                    fig = plt.figure(figsize=(img_size / dpi, img_size / dpi), dpi=dpi)\n",
    "                    ax = fig.gca()\n",
    "                    min_x, max_x, min_y, max_y = -5, 5, -6, 6\n",
    "                    ax.axis([min_x, max_x, min_y, max_y])\n",
    "                    ax.set_autoscale_on(False)  # allows us to define scale\n",
    "                    #plot lines for viewing\n",
    "                    x1 = range(-5,5)\n",
    "                    yleft = np.full((10,), points[top_left_index,:][1])\n",
    "                    yright = np.full((10,), points[top_right_index,:][1])\n",
    "                    ybottom_left_margin = np.full((10,), points[x_min_index,:][1]-margin)\n",
    "                    ybottom_right_margin = np.full((10,), points[x_max_index,:][1]-margin)\n",
    "                    ybottom_left = np.full((10,), points[x_min_index,:][1])\n",
    "                    ybottom_right = np.full((10,), points[x_max_index,:][1])\n",
    "                    \n",
    "                    # Display entire tooth\n",
    "                    segmented_points = points\n",
    "                    x = points[:,0]\n",
    "                    y = points[:,1]\n",
    "                    \n",
    "                    ax.plot(x, y,linewidth=1.0)\n",
    "                    ax.plot(x1, yleft, '-c')\n",
    "                    ax.plot(x1, yright, '-r')\n",
    "                    ax.plot(x1, ybottom_left_margin, '-py')\n",
    "                    ax.plot(x1, ybottom_left, '-y')\n",
    "                    ax.plot(x1, ybottom_right_margin, '-pb')\n",
    "                    ax.plot(x1, ybottom_right, '-b')\n",
    "                    \n",
    "                    fig.savefig(base_dir+\"/full/\"+file_name, bbox_inches='tight')\n",
    "                    plt.close()\n",
    "                    \n",
    "                    ax.relim()  \n",
    "                    ax.autoscale()\n",
    "                    \n",
    "                    #Left\n",
    "                    # If there is margin, choose points slightly below x_min_index\n",
    "                    if margin > 0:\n",
    "                        segmented_points = points[0:top_left_index+1,:]\n",
    "                        segmented_points = segmented_points[segmented_points[:,1]>points[x_min_index,1] - margin]\n",
    "                    else:\n",
    "                        segmented_points = points[x_min_index:top_left_index+1,:]\n",
    "                    x = segmented_points[:,0]\n",
    "                    y = segmented_points[:,1]\n",
    "                    \n",
    "                    ax.plot(x, y,linewidth=1.0)\n",
    "                    ax.plot(x1, yleft, '-c')\n",
    "                    ax.plot(x1, yright, '-r')\n",
    "                    ax.plot(x1, ybottom_left_margin, '-py')\n",
    "                    ax.plot(x1, ybottom_left, '-y')\n",
    "                    \n",
    "                    fig.savefig(base_dir+\"/left/\"+file_name, bbox_inches='tight')\n",
    "                    np.save(base_dir+\"/left_point/\"+file_name_point, segmented_points)\n",
    "                    plt.close()\n",
    "                    \n",
    "                    #Right\n",
    "                    if margin > 0:\n",
    "                        segmented_points = points[top_right_index:,:]\n",
    "                        segmented_points = segmented_points[segmented_points[:,1]>points[x_max_index, 1] - margin]\n",
    "                    else:\n",
    "                        segmented_points = points[top_right_index:x_max_index+1,:]\n",
    "                    x = segmented_points[:,0]\n",
    "                    y = segmented_points[:,1]\n",
    "                    \n",
    "                    ax.plot(x, y,linewidth=1.0)\n",
    "                    ax.plot(x1, yleft, '-c')\n",
    "                    ax.plot(x1, yright, '-r')\n",
    "                    ax.plot(x1, ybottom_right_margin, '-pb')\n",
    "                    ax.plot(x1, ybottom_right, '-b')\n",
    "                    \n",
    "                    fig.savefig(base_dir+\"/right/\"+file_name, bbox_inches='tight')\n",
    "                    np.save(base_dir+\"/right_point/\"+file_name_point, segmented_points)\n",
    "                    plt.close()\n",
    "                    \n",
    "                    #Top\n",
    "                    segmented_points = points[top_left_index:top_right_index+1,:]\n",
    "                    x = segmented_points[:,0]\n",
    "                    y = segmented_points[:,1]\n",
    "                    \n",
    "                    ax.plot(x, y,linewidth=1.0)\n",
    "                    ax.plot(x1, yleft, '-c')\n",
    "                    ax.plot(x1, yright, '-r')\n",
    "                    ax.plot(x1, ybottom_left_margin, '-py')\n",
    "                    ax.plot(x1, ybottom_left, '-y')\n",
    "                    \n",
    "                    fig.savefig(base_dir+\"/top/\"+file_name, bbox_inches='tight')\n",
    "                    np.save(base_dir+\"/top_point/\"+file_name_point, segmented_points)\n",
    "                    plt.close()\n",
    "            # plt.close()\n",
    "        if cnt%10 == 0:\n",
    "            print(\"Progress: %s, current image: %s\" % (cnt, im_name))\n",
    "                \n",
    "if __name__ == '__main__':\n",
    "    get_segment_multiple(name = '../global_data/stl_data', margin = 0, base_dir = \"/home/pasin/Documents/Google_Drive/Aa_TIT_LAB_Comp/Library/Tooth/Tooth/Model/my2DCNN/data/segment_2\")\n",
    "    print(\"Finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_slope(point1, point2):  # function to find top line\n",
    "    if point1[0] == point2[0]:\n",
    "        return 0\n",
    "    else:\n",
    "        return abs((point1[1] - point2[1]) / (point1[0] - point2[0]))\n",
    "\n",
    "print(find_slope(np.array([5,4]),np.array([5.0001,8])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "A = np.array([[5,6],[1,2],[3,4],[7,8],[9,10]])\n",
    "# print(A[0:2,:])\n",
    "# print(A[:,1]>3)\n",
    "print(A[A[:,1]>3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_save_file import get_file_name\n",
    "name_dir, image_name = get_file_name(folder_name='../global_data/stl_data', file_name=\"PreparationScan.stl\")\n",
    "print(image_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "np.save(\"/home/pasin/Documents/Link to Tooth/Tooth/Model/my2DCNN/data/test.npy\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.load(\"/home/pasin/Documents/Link to Tooth/Tooth/Model/my2DCNN/data/test.npy\")\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
