{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import time\n",
    "\n",
    "from ImportData2D import get_label, getFilename, get_file_name\n",
    "from stlSlicer import getSlicer, slicecoor, rotatestl\n",
    "from imgSave import saveplot\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(125)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = getLabel(\"Occ_Sum\", \"median\", True, False,False)\n",
    "print(type(label[0]))\n",
    "label = getLabel(\"Occ_Sum\", \"median\", True, True,False)\n",
    "print(type(label[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "test_name = \"prep_test.tfrecords\"\n",
    "test_name = os.path.join(\"data\", test_name)\n",
    "print(test_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "A = './data/preparation_181_data_test_eval_address.txt'\n",
    "print(os.path.abspath(A))\n",
    "print(os.path.isfile(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_grouped_address = ['/home/pasin/Documents/Google_Drive/Aa_TIT_LAB_Comp/Library/Tooth/Tooth/Model/my2DCNN/data/cross_section/PreparationScan_0_84140','/home/pasin/Documents/Google_Drive/Aa_TIT_LAB_Comp/Library/Tooth/Tooth/Model/my2DCNN/data/cross_section/PreparationScan_1_95138_0','/home/pasin/Documents/Google_Drive/Aa_TIT_LAB_Comp/Library/Tooth/Tooth/Model/my2DCNN/data/cross_section/PreparationScan_1_304138_45']\n",
    "grouped_address = [1,2,3]\n",
    "# define an empty list\n",
    "train_address = []\n",
    "\n",
    "# open file and read the content in a list\n",
    "with open(A, 'r') as filehandle:  \n",
    "    for line in filehandle:\n",
    "        # remove linebreak which is the last character of the string\n",
    "        current_name = line[:-1]\n",
    "        # print(current_name)\n",
    "        for i, name in enumerate(example_grouped_address):\n",
    "            \n",
    "            if current_name in name:\n",
    "                train_address.append(grouped_address[i])\n",
    "                grouped_address.remove(grouped_address[i])\n",
    "                example_grouped_address.remove(example_grouped_address[i])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(train_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "B = ast.literal_eval(error_file_names[0])\n",
    "print(B[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_address = ['abc_0_0','abc_0_90','abc_0_45','abc_0_135','abc_1_0','abc_1_45','abc_1_90','abc_1_70']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_address.sort()\n",
    "print([s for s in train_address if 'abc_0' in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import csv\n",
    "A = {'learning_rate':0.00009123,'keep_prob':0.25, 'activation':tf.nn.relu, 'channels':[16,16,32]}\n",
    "# A = {'a':'b','c':'d'}\n",
    "\n",
    "with open(\"output\"+\".csv\", \"w\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for key, val in A.items():\n",
    "        writer.writerow([key, val])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can add the parameter data_home to wherever to where you want to download your data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "((train_data, train_labels),(eval_data, eval_labels)) = tf.keras.datasets.mnist.load_data()\n",
    "train_data = train_data / np.float32(255)\n",
    "train_data = train_data.reshape(train_data.shape[0], -1)\n",
    "print(train_data.shape)\n",
    "train_labels = train_labels.astype(np.int32)  # not required\n",
    "print(train_labels.shape)\n",
    "# test_size: what proportion of original data is used for test set\n",
    "train_img, test_img, train_lbl, test_lbl = train_test_split(\n",
    "    train_data, train_labels, test_size=1/7.0, random_state=0)\n",
    "print(train_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the Data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training set only.\n",
    "scaler.fit(train_img)\n",
    "\n",
    "# Apply transform to both the training set and the test set.\n",
    "train_img = scaler.transform(train_img)\n",
    "test_img = scaler.transform(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "pca = PCA(.95)\n",
    "pca.fit(train_img)\n",
    "pca.n_components_\n",
    "train_img = pca.transform(train_img)\n",
    "test_img = pca.transform(test_img)\n",
    "\n",
    "# all parameters not specified are set to their defaults\n",
    "# default solver is incredibly slow thats why we change it\n",
    "# solver = 'lbfgs'\n",
    "logisticRegr = LogisticRegression(solver = 'lbfgs')\n",
    "\n",
    "logisticRegr.fit(train_img, train_lbl)\n",
    "\n",
    "# Returns a NumPy Array\n",
    "# Predict for One Observation (image)\n",
    "logisticRegr.predict(test_img[0].reshape(1,-1))\n",
    "\n",
    "# Predict for Multiple Observations (images) at Once\n",
    "logisticRegr.predict(test_img[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = logisticRegr.score(test_img, test_lbl)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/preparation_181_data_Taper_Sum_median_eval.tfrecords'\n",
    "arrays = get_data_from_path(data_path)\n",
    "# example, label = get_data_from_path(data_path)\n",
    "# print(example[0])\n",
    "print((arrays[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex, label = get_data_from_path(data_path)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex, label = get_data_from_path(data_path)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "csvData = [['Person', 'Age'], ['Peter', '22'], ['Jasmine', '21'], ['Sam', '24']]\n",
    "print(type(csvData))\n",
    "print((csvData))\n",
    "print(type(csvData[0]))\n",
    "print((csvData[0]))\n",
    "'''\n",
    "with open('person.csv', 'w') as csvFile:\n",
    "    writer = csv.writer(csvFile)\n",
    "    writer.writerows(csvData)\n",
    "\n",
    "csvFile.close()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvData = [(-0.53103447, [1e-05, 0.0, '1', 1]), (-0.53103447, [4.315202424267864e-05, 0.0, '1', 1]), (-0.53103447, [6.388961292348475e-05, 0.04720173848853115, '1', 4]), (-0.53103447, [0.0001540877645803547, 0.35419707255064065, '1', 1])]\n",
    "newData = []\n",
    "for i in csvData:\n",
    "    data = {'value': i[0],\n",
    "           'learning_rate': i[1][0],\n",
    "           'dropout_rate': i[1][1],\n",
    "           'activation': i[1][1],\n",
    "           'channels': i[1][1],}\n",
    "    newData.append(data)\n",
    "print(type(csvData))\n",
    "print((csvData))\n",
    "#print(type(csvData[0]))\n",
    "# print((csvData[0]))\n",
    "\n",
    "\n",
    "with open('person.csv', 'w') as csvFile:\n",
    "    writer = csv.DictWriter(csvFile, fieldnames = ['value','learning_rate','dropout_rate', 'activation', 'channels'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(newData)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Func vals: %s \\n\" % search_result.func_vals)\n",
    "print(\"Hyper para: %s \\n\" % search_result.x_iters)\n",
    "print(\"Combined: %s\" % list(zip(search_result.func_vals, search_result.x_iters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searched_parameter_2 = (list(zip(search_result.func_vals, search_result.x_iters)))\n",
    "i = searched_parameter_2[0]\n",
    "data_2 = {field_name[0]: i[0] * -1,\n",
    "                field_name[1]: i[1][0],\n",
    "                field_name[2]: i[1][1],\n",
    "                field_name[3]: i[1][1],\n",
    "                field_name[4]: i[1][1]}\n",
    "print(data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stl_points = points\n",
    "new_stl_points = []\n",
    "for stl_point_sample in stl_points:  # stl_point_sample is one example of stl_points\n",
    "    new_points_sample = []\n",
    "    for stl_point_image in stl_point_sample:  # stl_point_image is one degree of cross-section\n",
    "        difference = stl_point_image[1:, :] - stl_point_image[0:-1, :]  # Find difference between each position\n",
    "        new_points_sample.append(difference)\n",
    "    new_stl_points.append(new_points_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(new_stl_points))\n",
    "print(len(new_stl_points))\n",
    "print(type(new_stl_points[0]))\n",
    "print(len(new_stl_points[0]))\n",
    "print(type(new_stl_points[0][0]))\n",
    "print(np.shape(new_stl_points[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lbl_name)\n",
    "print(err_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# File name will be [tfrecord_name]_train_Taper_sum_median\n",
    "tfrecord_file_name = \"preparation_181_data\"\n",
    "# tfrecord_name = \"original_preparation_data\"\n",
    "# csv_name = \"../global_data/Ground Truth Score_50.csv\"\n",
    "# Directory of image\n",
    "dataset_folder_dir = \"./data/cross_section\"\n",
    " \n",
    "file_name = './data/preparation_181_data_train_address.txt'\n",
    "name = []\n",
    "if os.path.isfile(file_name):  # Check if file exist\n",
    "    with open(file_name) as f:\n",
    "        name = f.read().splitlines()\n",
    "    '''\n",
    "    with open(file_name, 'r') as filehandle:\n",
    "        for line in filehandle:\n",
    "            name.append(line)\n",
    "            \n",
    "            # remove linebreak which is the last character of the string\n",
    "            # current_name = line[:-1]\n",
    "            \n",
    "            # check if it exist in grouped exist, if found, put in train_address\n",
    "            for i, name in enumerate(example_grouped_address):\n",
    "                if current_name in name:\n",
    "                    train_address.append(grouped_address[i])\n",
    "                    grouped_address.remove(grouped_address[i])\n",
    "                    example_grouped_address.remove(example_grouped_address[i])\n",
    "                    break\n",
    "            '''\n",
    "    # print(\"Use %s examples from previous tfrecords as training\" % len(train_address))\n",
    "else:\n",
    "    print(\"Not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(name))\n",
    "for n in name[1:]:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (n[1] == '/home/pasin/Documents/Google_Drive/Aa_TIT_LAB_Comp/Library/Tooth/Tooth/Model/my2DCNN/data/cross_section/PreparationScan_1_85148')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "A = np.array([[1,2],[3,4],[5,6]])\n",
    "print((A.tolist()))\n",
    "print(A[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readjust_median_label(label, avg_data):\n",
    "    possible_value = [1,3,5]\n",
    "    for i, label_value in enumerate(label):\n",
    "        if not (label_value in possible_value):\n",
    "            # Check if value is over/under boundary, if so, choose the min/max value\n",
    "            if label_value < possible_value[0]:\n",
    "                label[i] = possible_value[0]\n",
    "            elif label_value > possible_value[-1]:\n",
    "                label[i] = possible_value[-1]\n",
    "            else:\n",
    "                if label_value > avg_data[i]:  # If median is more than average, round up\n",
    "                    label[i] = min(filter(lambda x: x > label_value, possible_value))\n",
    "                else:  # If median is less or equal to average, around down\n",
    "                    label[i] = max(filter(lambda x: x < label_value, possible_value))\n",
    "    return label\n",
    "\n",
    "A = [1,3,5,2,2,4,4, 10, -2, 2]\n",
    "B = [1,4,2,3,1,4,7, 15]\n",
    "\n",
    "C = readjust_median_label(A,B)\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.load('/home/pasin/Documents/Google_Drive/Aa_TIT_LAB_Comp/Library/Tooth/Tooth/Model/my2DCNN/data/coordinates/PreparationScan_1_305139_0.npy')\n",
    "print(type(data))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n",
      "<class 'str'>\n",
      "<class 'int'>\n",
      "1\n",
      "<class 'str'>\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "A = {'A':11,'B':21}\n",
    "print(len(A))\n",
    "C = [3,4]\n",
    "for i,(key,value) in enumerate(A.items()):\n",
    "    print(i)\n",
    "    print(type(key))\n",
    "    print(type(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c\n",
      "<class 'str'>\n",
      "1\n",
      "<class 'int'>\n",
      "b\n",
      "<class 'str'>\n",
      "2\n",
      "<class 'int'>\n",
      "a\n",
      "<class 'str'>\n",
      "3\n",
      "<class 'int'>\n",
      "{'c': 1, 'b': 2, 'a': 3}\n"
     ]
    }
   ],
   "source": [
    "def testFunction(**args):\n",
    "    B = {}\n",
    "    for key,value in args.items():\n",
    "        print(key)\n",
    "        print(type(key))\n",
    "        print(value)\n",
    "        print(type(value))\n",
    "        B[key] = value\n",
    "    return B\n",
    "B = testFunction(c=1,b=2,a =3)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: C not found, use default value = 50\n",
      "{'A': 11, 'B': 21, 'C': 50}\n"
     ]
    }
   ],
   "source": [
    "def check_exist(dictionary, **kwargs):\n",
    "    output_dict = dictionary\n",
    "    for key, value in kwargs.items():\n",
    "        try:\n",
    "            output_dict[key] = dictionary[key]\n",
    "            # output = params[dict_name]\n",
    "        except (KeyError, TypeError) as error:\n",
    "            if value is None:\n",
    "                raise Exception(\"Parameter '%s' not defined\" % key)\n",
    "            else:\n",
    "                output_dict[key] = value\n",
    "                print(\"Parameters: %s not found, use default value = %s\" % (key, value))\n",
    "    return output_dict\n",
    "\n",
    "X = {'A':11,'B':21}\n",
    "Y = check_exist(X,A=15,B=None,C=50)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'expand'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-d073e705b310>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'expand'"
     ]
    }
   ],
   "source": [
    "A = [1,2,3]\n",
    "B = [4,5]\n",
    "print(A.expand(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
